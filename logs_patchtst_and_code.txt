# ä¸¤ä»½logsçš„é¡ºåºå¯¹åº”ä¸‹è¾¹ä¸¤ä»½ä»£ç çš„é¡ºåº
(base) root@ubuntu22:~# torchrun --nproc_per_node=4 test3.py
[2026-02-15 00:18:25,122] torch.distributed.run: [WARNING] 
[2026-02-15 00:18:25,122] torch.distributed.run: [WARNING] *****************************************
[2026-02-15 00:18:25,122] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2026-02-15 00:18:25,122] torch.distributed.run: [WARNING] *****************************************
ðŸ“¥ Initializing (Downloading dataset if needed)...
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
ðŸš€ Pure PatchTST Started | Quiet Mode Active | HuberLoss(delta=1.0) | Epochs=30
âœ… Epoch 1/30 | Train HuberLoss: 0.1033 | Val MSE: 0.1707
âœ… Epoch 2/30 | Train HuberLoss: 0.0852 | Val MSE: 0.1680
âœ… Epoch 3/30 | Train HuberLoss: 0.0826 | Val MSE: 0.1631
âœ… Epoch 4/30 | Train HuberLoss: 0.0801 | Val MSE: 0.1522
âœ… Epoch 5/30 | Train HuberLoss: 0.0782 | Val MSE: 0.1511
âœ… Epoch 6/30 | Train HuberLoss: 0.0765 | Val MSE: 0.1505
âœ… Epoch 7/30 | Train HuberLoss: 0.0761 | Val MSE: 0.1484
âœ… Epoch 8/30 | Train HuberLoss: 0.0748 | Val MSE: 0.1443
âœ… Epoch 9/30 | Train HuberLoss: 0.0739 | Val MSE: 0.1444
âœ… Epoch 10/30 | Train HuberLoss: 0.0734 | Val MSE: 0.1427
âœ… Epoch 11/30 | Train HuberLoss: 0.0721 | Val MSE: 0.1425
âœ… Epoch 12/30 | Train HuberLoss: 0.0728 | Val MSE: 0.1397
âœ… Epoch 13/30 | Train HuberLoss: 0.0719 | Val MSE: 0.1388
âœ… Epoch 14/30 | Train HuberLoss: 0.0719 | Val MSE: 0.1402
âœ… Epoch 15/30 | Train HuberLoss: 0.0714 | Val MSE: 0.1370
âœ… Epoch 16/30 | Train HuberLoss: 0.0707 | Val MSE: 0.1368
âœ… Epoch 17/30 | Train HuberLoss: 0.0700 | Val MSE: 0.1369
âœ… Epoch 18/30 | Train HuberLoss: 0.0698 | Val MSE: 0.1362
âœ… Epoch 19/30 | Train HuberLoss: 0.0697 | Val MSE: 0.1374
âœ… Epoch 20/30 | Train HuberLoss: 0.0698 | Val MSE: 0.1350
âœ… Epoch 21/30 | Train HuberLoss: 0.0686 | Val MSE: 0.1361
âœ… Epoch 22/30 | Train HuberLoss: 0.0688 | Val MSE: 0.1355
âœ… Epoch 23/30 | Train HuberLoss: 0.0688 | Val MSE: 0.1339
âœ… Epoch 24/30 | Train HuberLoss: 0.0678 | Val MSE: 0.1346
âœ… Epoch 25/30 | Train HuberLoss: 0.0681 | Val MSE: 0.1333
âœ… Epoch 26/30 | Train HuberLoss: 0.0680 | Val MSE: 0.1338
âœ… Epoch 27/30 | Train HuberLoss: 0.0680 | Val MSE: 0.1335
âœ… Epoch 28/30 | Train HuberLoss: 0.0671 | Val MSE: 0.1333
âœ… Epoch 29/30 | Train HuberLoss: 0.0671 | Val MSE: 0.1334
âœ… Epoch 30/30 | Train HuberLoss: 0.0679 | Val MSE: 0.1333


(base) root@ubuntu22:~# torchrun --nproc_per_node=4 test3.py
[2026-02-15 00:26:10,889] torch.distributed.run: [WARNING] 
[2026-02-15 00:26:10,889] torch.distributed.run: [WARNING] *****************************************
[2026-02-15 00:26:10,889] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2026-02-15 00:26:10,889] torch.distributed.run: [WARNING] *****************************************
ðŸ“¥ Initializing (Downloading dataset if needed)...
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
ðŸš€ Native PatchTST Started | Quiet Mode Active | MSE Loss Only | Epochs=30
âœ… Epoch 1/30 | Train MSE: 0.2369 | Val MSE: 0.1716
âœ… Epoch 2/30 | Train MSE: 0.1926 | Val MSE: 0.1696
âœ… Epoch 3/30 | Train MSE: 0.1853 | Val MSE: 0.1613
âœ… Epoch 4/30 | Train MSE: 0.1782 | Val MSE: 0.1522
âœ… Epoch 5/30 | Train MSE: 0.1741 | Val MSE: 0.1532
âœ… Epoch 6/30 | Train MSE: 0.1706 | Val MSE: 0.1518
âœ… Epoch 7/30 | Train MSE: 0.1698 | Val MSE: 0.1499
âœ… Epoch 8/30 | Train MSE: 0.1666 | Val MSE: 0.1449
âœ… Epoch 9/30 | Train MSE: 0.1643 | Val MSE: 0.1449
âœ… Epoch 10/30 | Train MSE: 0.1632 | Val MSE: 0.1444
âœ… Epoch 11/30 | Train MSE: 0.1598 | Val MSE: 0.1423
âœ… Epoch 12/30 | Train MSE: 0.1611 | Val MSE: 0.1403
âœ… Epoch 13/30 | Train MSE: 0.1591 | Val MSE: 0.1399
âœ… Epoch 14/30 | Train MSE: 0.1591 | Val MSE: 0.1406
âœ… Epoch 15/30 | Train MSE: 0.1578 | Val MSE: 0.1380
âœ… Epoch 16/30 | Train MSE: 0.1564 | Val MSE: 0.1380
âœ… Epoch 17/30 | Train MSE: 0.1550 | Val MSE: 0.1380
âœ… Epoch 18/30 | Train MSE: 0.1544 | Val MSE: 0.1371
âœ… Epoch 19/30 | Train MSE: 0.1542 | Val MSE: 0.1385
âœ… Epoch 20/30 | Train MSE: 0.1546 | Val MSE: 0.1362
âœ… Epoch 21/30 | Train MSE: 0.1514 | Val MSE: 0.1382
âœ… Epoch 22/30 | Train MSE: 0.1523 | Val MSE: 0.1365
âœ… Epoch 23/30 | Train MSE: 0.1521 | Val MSE: 0.1351
âœ… Epoch 24/30 | Train MSE: 0.1497 | Val MSE: 0.1355
âœ… Epoch 25/30 | Train MSE: 0.1505 | Val MSE: 0.1346
âœ… Epoch 26/30 | Train MSE: 0.1501 | Val MSE: 0.1352
âœ… Epoch 27/30 | Train MSE: 0.1502 | Val MSE: 0.1349
âœ… Epoch 28/30 | Train MSE: 0.1481 | Val MSE: 0.1348
âœ… Epoch 29/30 | Train MSE: 0.1480 | Val MSE: 0.1347
âœ… Epoch 30/30 | Train MSE: 0.1500 | Val MSE: 0.1347




# import os, math, random, time, glob, torch, kagglehub
# import numpy as np
# import pandas as pd
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.distributed as dist
# from torch.nn.parallel import DistributedDataParallel as DDP
# from torch.utils.data.distributed import DistributedSampler
# from torch.utils.data import Dataset, DataLoader
# from torch.amp import autocast
# from sklearn.preprocessing import StandardScaler

# # ==========================================
# # 0. çŽ¯å¢ƒé…ç½®ä¸ŽåŠ é€Ÿ
# # ==========================================
# torch.backends.cuda.matmul.allow_tf32 = True
# torch.backends.cudnn.allow_tf32 = True
# torch.backends.cudnn.benchmark = True

# # ==========================================
# # 1. æ ¸å¿ƒç»„ä»¶ï¼šRevIN 
# # ==========================================
# class RevIN(nn.Module):
#     def __init__(self, num_features: int, eps=1e-5, affine=True):
#         super(RevIN, self).__init__()
#         self.num_features = num_features
#         self.eps = eps
#         self.affine = affine
#         if self.affine: self._init_params()

#     def _init_params(self):
#         self.affine_weight = nn.Parameter(torch.ones(self.num_features))
#         self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

#     def forward(self, x, mode:str):
#         if mode == 'norm':
#             self._get_statistics(x)
#             x = self._normalize(x)
#         elif mode == 'denorm':
#             x = self._denormalize(x)
#         return x

#     def _get_statistics(self, x):
#         dim2reduce = tuple(range(len(x.shape)-1))
#         self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
#         self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()

#     def _normalize(self, x):
#         x = (x - self.mean) / self.stdev
#         if self.affine: x = x * self.affine_weight + self.affine_bias
#         return x

#     def _denormalize(self, x):
#         if self.affine:
#             x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)
#         x = x * self.stdev + self.mean
#         return x

# # ==========================================
# # 2. åŽŸç”Ÿ PatchTST æž¶æž„
# # ==========================================
# class PatchTST(nn.Module):
#     def __init__(self, seq_len=96, pred_len=96, num_vars=321, patch_len=16, stride=8, d_model=128, n_heads=8, e_layers=3, d_ff=512, dropout=0.1):
#         super().__init__()
#         self.seq_len = seq_len
#         self.pred_len = pred_len
#         self.num_vars = num_vars
#         self.patch_len = patch_len
#         self.stride = stride
        
#         self.revin = RevIN(num_vars)
        
#         # è®¡ç®—åˆ‡ç‰‡(Patch)çš„æ•°é‡
#         self.num_patches = (seq_len - patch_len) // stride + 1
        
#         # Patch æ˜ å°„å±‚ä¸Žä½ç½®ç¼–ç 
#         self.patch_embedding = nn.Linear(patch_len, d_model)
#         self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, d_model))
        
#         # Transformer ç¼–ç å™¨
#         encoder_layer = nn.TransformerEncoderLayer(
#             d_model=d_model, 
#             nhead=n_heads, 
#             dim_feedforward=d_ff, 
#             dropout=dropout, 
#             batch_first=True, 
#             norm_first=True
#         )
#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)
        
#         # é¢„æµ‹å¤´ (å°†æ‰€æœ‰ Patch çš„ç‰¹å¾å±•å¹³åŽæ˜ å°„åˆ°é¢„æµ‹é•¿åº¦)
#         self.head = nn.Linear(self.num_patches * d_model, pred_len)

#     def forward(self, x):
#         # x shape: [B, Seq_Len, Num_Vars] 
#         x = self.revin(x, 'norm')
#         B, L, N = x.shape
        
#         # ðŸŒŸ é‡ç‚¹ 1ï¼šé€šé“ç‹¬ç«‹ (Channel Independence)
#         # å½¢çŠ¶å˜ä¸º [B*N, Seq_Len]ï¼Œæ¯ä¸ªå˜é‡è¢«å½“æˆç‹¬ç«‹çš„æ—¶é—´åºåˆ—
#         x = x.permute(0, 2, 1).reshape(B * N, L)
        
#         # ðŸŒŸ é‡ç‚¹ 2ï¼šåˆ‡ç‰‡ (Patching)
#         # ä½¿ç”¨ unfold æ»‘åŠ¨çª—å£åˆ‡å‰²åºåˆ— -> [B*N, num_patches, patch_len]
#         x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        
#         # åµŒå…¥ä¸Žä½ç½®ç¼–ç : [B*N, num_patches, d_model]
#         x = self.patch_embedding(x) + self.position_embedding
        
#         # Transformer å¤„ç†
#         x = self.encoder(x)
        
#         # å±•å¹³å¹¶é¢„æµ‹æœªæ¥: [B*N, num_patches * d_model] -> [B*N, pred_len]
#         x = x.reshape(B * N, -1)
#         out = self.head(x) 
        
#         # è¿˜åŽŸç»´åº¦: [B*N, pred_len] -> [B, pred_len, Num_Vars]
#         out = out.view(B, N, self.pred_len).permute(0, 2, 1)
#         out = self.revin(out, 'denorm')
#         return out

# # ==========================================
# # 3. æ•°æ®é›†å®šä¹‰ 
# # ==========================================
# class ElectricityDataset(Dataset):
#     def __init__(self, csv_path, flag='train'):
#         self.seq_len = 96; self.pred_len = 96
#         df_raw = pd.read_csv(csv_path)
#         df_data = df_raw.iloc[:, 1:] 
        
#         n = len(df_data)
#         num_train = int(n * 0.7); num_test = int(n * 0.2); num_val = n - num_train - num_test
        
#         border1s = [0, num_train - self.seq_len, n - num_test - self.seq_len]
#         border2s = [num_train, num_train + num_val, n]
        
#         f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
#         b1, b2 = border1s[f_idx], border2s[f_idx]
        
#         self.scaler = StandardScaler()
#         train_data = df_data.iloc[border1s[0]:border2s[0]]
#         self.scaler.fit(train_data.values)
#         data = self.scaler.transform(df_data.values)
        
#         self.data_x = data[b1:b2]
#         self.data_y = data[b1:b2]
    
#     def __getitem__(self, index):
#         s_begin = index; s_end = s_begin + self.seq_len
#         r_begin = s_end; r_end = r_begin + self.pred_len
#         return torch.tensor(self.data_x[s_begin:s_end], dtype=torch.float32), \
#                torch.tensor(self.data_y[r_begin:r_end], dtype=torch.float32)

#     def __len__(self): return len(self.data_x) - self.seq_len - self.pred_len + 1

# # ==========================================
# # 4. ä¸»è®­ç»ƒå¾ªçŽ¯ 
# # ==========================================
# def main():
#     if "LOCAL_RANK" in os.environ:
#         rank = int(os.environ["LOCAL_RANK"]); torch.cuda.set_device(rank)
#         dist.init_process_group("nccl"); world_size = dist.get_world_size()
#     else: rank = 0; world_size = 1; torch.cuda.set_device(0); dist.init_process_group("gloo", rank=0, world_size=1)

#     # ç‹¬ç«‹æ—¥å¿—æ–‡ä»¶
#     log_file = "patchtst_log.csv"

#     dataset_path_file = ".kaggle_path.tmp"
#     if rank == 0:
#         print("ðŸ“¥ Initializing (Downloading dataset if needed)...")
#         path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
#         with open(dataset_path_file, "w") as f: f.write(path)
        
#         if not os.path.exists(log_file):
#             with open(log_file, "w") as f:
#                 f.write("Epoch,Train_HuberLoss,Val_MSE\n")
    
#     if dist.is_initialized(): dist.barrier()
    
#     with open(dataset_path_file, "r") as f:
#         base_path = f.read().strip()
#     csv_path = glob.glob(os.path.join(base_path, "**", "electricity.csv"), recursive=True)[0]

#     # PatchTST æ³¨æ„åŠ›çŸ©é˜µæžå°ï¼Œéžå¸¸çœæ˜¾å­˜ï¼ŒBS è®¾ä¸º 64 (æ¯å¡ 16) å®Œå…¨æ²¡é—®é¢˜
#     BS = 64 // world_size
#     train_ds = ElectricityDataset(csv_path, 'train')
#     val_ds = ElectricityDataset(csv_path, 'val')
    
#     train_loader = DataLoader(train_ds, batch_size=BS, sampler=DistributedSampler(train_ds) if world_size > 1 else None, shuffle=(world_size==1), num_workers=4)
#     val_loader = DataLoader(val_ds, batch_size=BS, sampler=DistributedSampler(val_ds, shuffle=False) if world_size > 1 else None, shuffle=False)

#     # åˆå§‹åŒ–åŽŸç”Ÿ PatchTST
#     model = PatchTST(num_vars=321, d_model=128, n_heads=8, e_layers=3).to(rank)
#     if world_size > 1: model = DDP(model, device_ids=[rank])

#     optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30) 
    
#     # ðŸŒŸ å®Œå…¨å¯¹é½ä½ çš„è®¾å®šï¼šHuber Loss (delta=1.0) è®­ç»ƒ
#     criterion = nn.HuberLoss(delta=1.0) 

#     if rank == 0: print(f"ðŸš€ Pure PatchTST Started | Quiet Mode Active | HuberLoss(delta=1.0) | Epochs=30")
    
#     for epoch in range(30): 
#         if world_size > 1: train_loader.sampler.set_epoch(epoch)
#         model.train()
#         train_loss = 0
#         for bx, by in train_loader:
#             bx, by = bx.to(rank), by.to(rank)
#             optimizer.zero_grad(set_to_none=True)
#             with autocast(device_type='cuda', dtype=torch.bfloat16):
#                 pred = model(bx)
#                 loss = criterion(pred, by)
#             loss.backward()
#             optimizer.step()
#             train_loss += loss.item()
            
#         model.eval()
#         v_mse = 0; count = 0
#         with torch.no_grad():
#             for bx, by in val_loader:
#                 bx, by = bx.to(rank), by.to(rank)
#                 pred = model(bx)
#                 # ðŸŒŸ å®Œå…¨å¯¹é½ï¼šMSE éªŒè¯
#                 v_mse += F.mse_loss(pred, by).item() 
#                 count += 1
                
#         avg_mse = torch.tensor(v_mse/count).to(rank)
#         if world_size > 1: dist.all_reduce(avg_mse, op=dist.ReduceOp.SUM); avg_mse /= world_size

#         if rank == 0:
#             avg_train_loss = train_loss/len(train_loader)
#             print(f"âœ… Epoch {epoch+1}/30 | Train HuberLoss: {avg_train_loss:.4f} | Val MSE: {avg_mse.item():.4f}")
            
#             with open(log_file, "a") as f:
#                 f.write(f"{epoch+1},{avg_train_loss:.4f},{avg_mse.item():.4f}\n")
                
#             torch.save(model.state_dict(), "patchtst_best.pth")
        
#         scheduler.step()

# if __name__ == "__main__": 
#     main()

import os, math, random, time, glob, torch, kagglehub
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
from sklearn.preprocessing import StandardScaler

# ==========================================
# 0. çŽ¯å¢ƒé…ç½®ä¸ŽåŠ é€Ÿ
# ==========================================
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# ==========================================
# 1. æ ¸å¿ƒç»„ä»¶ï¼šRevIN 
# ==========================================
class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True):
        super(RevIN, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.affine = affine
        if self.affine: self._init_params()

    def _init_params(self):
        self.affine_weight = nn.Parameter(torch.ones(self.num_features))
        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

    def forward(self, x, mode:str):
        if mode == 'norm':
            self._get_statistics(x)
            x = self._normalize(x)
        elif mode == 'denorm':
            x = self._denormalize(x)
        return x

    def _get_statistics(self, x):
        dim2reduce = tuple(range(len(x.shape)-1))
        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()

    def _normalize(self, x):
        x = (x - self.mean) / self.stdev
        if self.affine: x = x * self.affine_weight + self.affine_bias
        return x

    def _denormalize(self, x):
        if self.affine:
            x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)
        x = x * self.stdev + self.mean
        return x

# ==========================================
# 2. åŽŸç”Ÿ PatchTST æž¶æž„
# ==========================================
class PatchTST(nn.Module):
    def __init__(self, seq_len=96, pred_len=96, num_vars=321, patch_len=16, stride=8, d_model=128, n_heads=8, e_layers=3, d_ff=512, dropout=0.1):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.num_vars = num_vars
        self.patch_len = patch_len
        self.stride = stride
        
        self.revin = RevIN(num_vars)
        
        # è®¡ç®—åˆ‡ç‰‡(Patch)çš„æ•°é‡: å¯¹äºŽ 96 é•¿åº¦ï¼Œpatch=16, stride=8ï¼Œä¼šåˆ‡å‡º 11 ä¸ª patch
        self.num_patches = (seq_len - patch_len) // stride + 1
        
        # Patch æ˜ å°„å±‚ä¸Žä½ç½®ç¼–ç 
        self.patch_embedding = nn.Linear(patch_len, d_model)
        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, d_model))
        
        # Transformer ç¼–ç å™¨ (ä»…å¤„ç†æ—¶é—´ç»´åº¦ï¼Œé€šé“å®Œå…¨ç‹¬ç«‹)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=n_heads, 
            dim_feedforward=d_ff, 
            dropout=dropout, 
            batch_first=True, 
            norm_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)
        
        # é¢„æµ‹å¤´ (å°†æ‰€æœ‰ Patch çš„ç‰¹å¾å±•å¹³åŽç›´æŽ¥æ˜ å°„åˆ°æœªæ¥é•¿åº¦)
        self.head = nn.Linear(self.num_patches * d_model, pred_len)

    def forward(self, x):
        # x shape: [B, Seq_Len, Num_Vars] 
        x = self.revin(x, 'norm')
        B, L, N = x.shape
        
        # 1. é€šé“ç‹¬ç«‹ (Channel Independence)
        # æŠŠæ¯ä¸ªå˜é‡å•ç‹¬æ‹Žå‡ºæ¥: [B, L, N] -> [B, N, L] -> [B*N, L]
        x = x.permute(0, 2, 1).reshape(B * N, L)
        
        # 2. åˆ‡ç‰‡ (Patching)
        # ç”¨ unfold åˆ‡å‰²: [B*N, num_patches, patch_len]
        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        
        # 3. åµŒå…¥ä¸Žä½ç½®ç¼–ç : [B*N, num_patches, d_model]
        x = self.patch_embedding(x) + self.position_embedding
        
        # 4. Transformer å¤„ç†æ—¶é—´ä¾èµ–
        x = self.encoder(x)
        
        # 5. å±•å¹³å¹¶é¢„æµ‹: [B*N, num_patches * d_model] -> [B*N, pred_len]
        x = x.reshape(B * N, -1)
        out = self.head(x) 
        
        # 6. è¿˜åŽŸç»´åº¦å¹¶åå½’ä¸€åŒ–: [B*N, pred_len] -> [B, pred_len, Num_Vars]
        out = out.view(B, N, self.pred_len).permute(0, 2, 1)
        out = self.revin(out, 'denorm')
        return out

# ==========================================
# 3. æ•°æ®é›†å®šä¹‰ 
# ==========================================
class ElectricityDataset(Dataset):
    def __init__(self, csv_path, flag='train'):
        self.seq_len = 96; self.pred_len = 96
        df_raw = pd.read_csv(csv_path)
        df_data = df_raw.iloc[:, 1:] 
        
        n = len(df_data)
        num_train = int(n * 0.7); num_test = int(n * 0.2); num_val = n - num_train - num_test
        
        border1s = [0, num_train - self.seq_len, n - num_test - self.seq_len]
        border2s = [num_train, num_train + num_val, n]
        
        f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
        b1, b2 = border1s[f_idx], border2s[f_idx]
        
        self.scaler = StandardScaler()
        train_data = df_data.iloc[border1s[0]:border2s[0]]
        self.scaler.fit(train_data.values)
        data = self.scaler.transform(df_data.values)
        
        self.data_x = data[b1:b2]
        self.data_y = data[b1:b2]
    
    def __getitem__(self, index):
        s_begin = index; s_end = s_begin + self.seq_len
        r_begin = s_end; r_end = r_begin + self.pred_len
        return torch.tensor(self.data_x[s_begin:s_end], dtype=torch.float32), \
               torch.tensor(self.data_y[r_begin:r_end], dtype=torch.float32)

    def __len__(self): return len(self.data_x) - self.seq_len - self.pred_len + 1

# ==========================================
# 4. ä¸»è®­ç»ƒå¾ªçŽ¯ 
# ==========================================
def main():
    if "LOCAL_RANK" in os.environ:
        rank = int(os.environ["LOCAL_RANK"]); torch.cuda.set_device(rank)
        dist.init_process_group("nccl"); world_size = dist.get_world_size()
    else: rank = 0; world_size = 1; torch.cuda.set_device(0); dist.init_process_group("gloo", rank=0, world_size=1)

    log_file = "patchtst_mse_log.csv"

    dataset_path_file = ".kaggle_path.tmp"
    if rank == 0:
        print("ðŸ“¥ Initializing (Downloading dataset if needed)...")
        path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
        with open(dataset_path_file, "w") as f: f.write(path)
        
        if not os.path.exists(log_file):
            with open(log_file, "w") as f:
                # è®°å½•å¤´æ”¹å›ž Train_MSE
                f.write("Epoch,Train_MSE,Val_MSE\n")
    
    if dist.is_initialized(): dist.barrier()
    
    with open(dataset_path_file, "r") as f:
        base_path = f.read().strip()
    csv_path = glob.glob(os.path.join(base_path, "**", "electricity.csv"), recursive=True)[0]

    BS = 64 // world_size
    train_ds = ElectricityDataset(csv_path, 'train')
    val_ds = ElectricityDataset(csv_path, 'val')
    
    train_loader = DataLoader(train_ds, batch_size=BS, sampler=DistributedSampler(train_ds) if world_size > 1 else None, shuffle=(world_size==1), num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=BS, sampler=DistributedSampler(val_ds, shuffle=False) if world_size > 1 else None, shuffle=False)

    model = PatchTST(num_vars=321, d_model=128, n_heads=8, e_layers=3).to(rank)
    if world_size > 1: model = DDP(model, device_ids=[rank])

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30) 
    
    # ðŸŒŸ é‡ç‚¹æ›¿æ¢ï¼šè®­ç»ƒä¹Ÿä½¿ç”¨åŽŸç”Ÿ MSE Loss
    criterion = nn.MSELoss() 

    if rank == 0: print(f"ðŸš€ Native PatchTST Started | Quiet Mode Active | MSE Loss Only | Epochs=30")
    
    for epoch in range(30): 
        if world_size > 1: train_loader.sampler.set_epoch(epoch)
        model.train()
        train_loss = 0
        for bx, by in train_loader:
            bx, by = bx.to(rank), by.to(rank)
            optimizer.zero_grad(set_to_none=True)
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                pred = model(bx)
                loss = criterion(pred, by) # ä½¿ç”¨ MSE è®¡ç®—æ¢¯åº¦
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        model.eval()
        v_mse = 0; count = 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(rank), by.to(rank)
                pred = model(bx)
                v_mse += F.mse_loss(pred, by).item() 
                count += 1
                
        avg_mse = torch.tensor(v_mse/count).to(rank)
        if world_size > 1: dist.all_reduce(avg_mse, op=dist.ReduceOp.SUM); avg_mse /= world_size

        if rank == 0:
            avg_train_loss = train_loss/len(train_loader)
            print(f"âœ… Epoch {epoch+1}/30 | Train MSE: {avg_train_loss:.4f} | Val MSE: {avg_mse.item():.4f}")
            
            with open(log_file, "a") as f:
                f.write(f"{epoch+1},{avg_train_loss:.4f},{avg_mse.item():.4f}\n")
                
            torch.save(model.state_dict(), "patchtst_native_best.pth")
        
        scheduler.step()

if __name__ == "__main__": main()


