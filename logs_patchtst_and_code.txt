# ‰∏§‰ªΩlogsÁöÑÈ°∫Â∫èÂØπÂ∫î‰∏ãËæπ‰∏§‰ªΩ‰ª£Á†ÅÁöÑÈ°∫Â∫è
(base) root@ubuntu22:~# torchrun --nproc_per_node=4 test3.py
[2026-02-15 00:18:25,122] torch.distributed.run: [WARNING] 
[2026-02-15 00:18:25,122] torch.distributed.run: [WARNING] *****************************************
[2026-02-15 00:18:25,122] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2026-02-15 00:18:25,122] torch.distributed.run: [WARNING] *****************************************
üì• Initializing (Downloading dataset if needed)...
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
üöÄ Pure PatchTST Started | Quiet Mode Active | HuberLoss(delta=1.0) | Epochs=30
‚úÖ Epoch 1/30 | Train HuberLoss: 0.1033 | Val MSE: 0.1707
‚úÖ Epoch 2/30 | Train HuberLoss: 0.0852 | Val MSE: 0.1680
‚úÖ Epoch 3/30 | Train HuberLoss: 0.0826 | Val MSE: 0.1631
‚úÖ Epoch 4/30 | Train HuberLoss: 0.0801 | Val MSE: 0.1522
‚úÖ Epoch 5/30 | Train HuberLoss: 0.0782 | Val MSE: 0.1511
‚úÖ Epoch 6/30 | Train HuberLoss: 0.0765 | Val MSE: 0.1505
‚úÖ Epoch 7/30 | Train HuberLoss: 0.0761 | Val MSE: 0.1484
‚úÖ Epoch 8/30 | Train HuberLoss: 0.0748 | Val MSE: 0.1443
‚úÖ Epoch 9/30 | Train HuberLoss: 0.0739 | Val MSE: 0.1444
‚úÖ Epoch 10/30 | Train HuberLoss: 0.0734 | Val MSE: 0.1427
‚úÖ Epoch 11/30 | Train HuberLoss: 0.0721 | Val MSE: 0.1425
‚úÖ Epoch 12/30 | Train HuberLoss: 0.0728 | Val MSE: 0.1397
‚úÖ Epoch 13/30 | Train HuberLoss: 0.0719 | Val MSE: 0.1388
‚úÖ Epoch 14/30 | Train HuberLoss: 0.0719 | Val MSE: 0.1402
‚úÖ Epoch 15/30 | Train HuberLoss: 0.0714 | Val MSE: 0.1370
‚úÖ Epoch 16/30 | Train HuberLoss: 0.0707 | Val MSE: 0.1368
‚úÖ Epoch 17/30 | Train HuberLoss: 0.0700 | Val MSE: 0.1369
‚úÖ Epoch 18/30 | Train HuberLoss: 0.0698 | Val MSE: 0.1362
‚úÖ Epoch 19/30 | Train HuberLoss: 0.0697 | Val MSE: 0.1374
‚úÖ Epoch 20/30 | Train HuberLoss: 0.0698 | Val MSE: 0.1350
‚úÖ Epoch 21/30 | Train HuberLoss: 0.0686 | Val MSE: 0.1361
‚úÖ Epoch 22/30 | Train HuberLoss: 0.0688 | Val MSE: 0.1355
‚úÖ Epoch 23/30 | Train HuberLoss: 0.0688 | Val MSE: 0.1339
‚úÖ Epoch 24/30 | Train HuberLoss: 0.0678 | Val MSE: 0.1346
‚úÖ Epoch 25/30 | Train HuberLoss: 0.0681 | Val MSE: 0.1333
‚úÖ Epoch 26/30 | Train HuberLoss: 0.0680 | Val MSE: 0.1338
‚úÖ Epoch 27/30 | Train HuberLoss: 0.0680 | Val MSE: 0.1335
‚úÖ Epoch 28/30 | Train HuberLoss: 0.0671 | Val MSE: 0.1333
‚úÖ Epoch 29/30 | Train HuberLoss: 0.0671 | Val MSE: 0.1334
‚úÖ Epoch 30/30 | Train HuberLoss: 0.0679 | Val MSE: 0.1333


(base) root@ubuntu22:~# torchrun --nproc_per_node=4 test3.py
[2026-02-15 00:26:10,889] torch.distributed.run: [WARNING] 
[2026-02-15 00:26:10,889] torch.distributed.run: [WARNING] *****************************************
[2026-02-15 00:26:10,889] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2026-02-15 00:26:10,889] torch.distributed.run: [WARNING] *****************************************
üì• Initializing (Downloading dataset if needed)...
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
üöÄ Native PatchTST Started | Quiet Mode Active | MSE Loss Only | Epochs=30
‚úÖ Epoch 1/30 | Train MSE: 0.2369 | Val MSE: 0.1716
‚úÖ Epoch 2/30 | Train MSE: 0.1926 | Val MSE: 0.1696
‚úÖ Epoch 3/30 | Train MSE: 0.1853 | Val MSE: 0.1613
‚úÖ Epoch 4/30 | Train MSE: 0.1782 | Val MSE: 0.1522
‚úÖ Epoch 5/30 | Train MSE: 0.1741 | Val MSE: 0.1532
‚úÖ Epoch 6/30 | Train MSE: 0.1706 | Val MSE: 0.1518
‚úÖ Epoch 7/30 | Train MSE: 0.1698 | Val MSE: 0.1499
‚úÖ Epoch 8/30 | Train MSE: 0.1666 | Val MSE: 0.1449
‚úÖ Epoch 9/30 | Train MSE: 0.1643 | Val MSE: 0.1449
‚úÖ Epoch 10/30 | Train MSE: 0.1632 | Val MSE: 0.1444
‚úÖ Epoch 11/30 | Train MSE: 0.1598 | Val MSE: 0.1423
‚úÖ Epoch 12/30 | Train MSE: 0.1611 | Val MSE: 0.1403
‚úÖ Epoch 13/30 | Train MSE: 0.1591 | Val MSE: 0.1399
‚úÖ Epoch 14/30 | Train MSE: 0.1591 | Val MSE: 0.1406
‚úÖ Epoch 15/30 | Train MSE: 0.1578 | Val MSE: 0.1380
‚úÖ Epoch 16/30 | Train MSE: 0.1564 | Val MSE: 0.1380
‚úÖ Epoch 17/30 | Train MSE: 0.1550 | Val MSE: 0.1380
‚úÖ Epoch 18/30 | Train MSE: 0.1544 | Val MSE: 0.1371
‚úÖ Epoch 19/30 | Train MSE: 0.1542 | Val MSE: 0.1385
‚úÖ Epoch 20/30 | Train MSE: 0.1546 | Val MSE: 0.1362
‚úÖ Epoch 21/30 | Train MSE: 0.1514 | Val MSE: 0.1382
‚úÖ Epoch 22/30 | Train MSE: 0.1523 | Val MSE: 0.1365
‚úÖ Epoch 23/30 | Train MSE: 0.1521 | Val MSE: 0.1351
‚úÖ Epoch 24/30 | Train MSE: 0.1497 | Val MSE: 0.1355
‚úÖ Epoch 25/30 | Train MSE: 0.1505 | Val MSE: 0.1346
‚úÖ Epoch 26/30 | Train MSE: 0.1501 | Val MSE: 0.1352
‚úÖ Epoch 27/30 | Train MSE: 0.1502 | Val MSE: 0.1349
‚úÖ Epoch 28/30 | Train MSE: 0.1481 | Val MSE: 0.1348
‚úÖ Epoch 29/30 | Train MSE: 0.1480 | Val MSE: 0.1347
‚úÖ Epoch 30/30 | Train MSE: 0.1500 | Val MSE: 0.1347




# import os, math, random, time, glob, torch, kagglehub
# import numpy as np
# import pandas as pd
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.distributed as dist
# from torch.nn.parallel import DistributedDataParallel as DDP
# from torch.utils.data.distributed import DistributedSampler
# from torch.utils.data import Dataset, DataLoader
# from torch.amp import autocast
# from sklearn.preprocessing import StandardScaler

# # ==========================================
# # 0. ÁéØÂ¢ÉÈÖçÁΩÆ‰∏éÂä†ÈÄü
# # ==========================================
# torch.backends.cuda.matmul.allow_tf32 = True
# torch.backends.cudnn.allow_tf32 = True
# torch.backends.cudnn.benchmark = True

# # ==========================================
# # 1. Ê†∏ÂøÉÁªÑ‰ª∂ÔºöRevIN 
# # ==========================================
# class RevIN(nn.Module):
#     def __init__(self, num_features: int, eps=1e-5, affine=True):
#         super(RevIN, self).__init__()
#         self.num_features = num_features
#         self.eps = eps
#         self.affine = affine
#         if self.affine: self._init_params()

#     def _init_params(self):
#         self.affine_weight = nn.Parameter(torch.ones(self.num_features))
#         self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

#     def forward(self, x, mode:str):
#         if mode == 'norm':
#             self._get_statistics(x)
#             x = self._normalize(x)
#         elif mode == 'denorm':
#             x = self._denormalize(x)
#         return x

#     def _get_statistics(self, x):
#         dim2reduce = tuple(range(len(x.shape)-1))
#         self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
#         self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()

#     def _normalize(self, x):
#         x = (x - self.mean) / self.stdev
#         if self.affine: x = x * self.affine_weight + self.affine_bias
#         return x

#     def _denormalize(self, x):
#         if self.affine:
#             x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)
#         x = x * self.stdev + self.mean
#         return x

# # ==========================================
# # 2. ÂéüÁîü PatchTST Êû∂ÊûÑ
# # ==========================================
# class PatchTST(nn.Module):
#     def __init__(self, seq_len=96, pred_len=96, num_vars=321, patch_len=16, stride=8, d_model=128, n_heads=8, e_layers=3, d_ff=512, dropout=0.1):
#         super().__init__()
#         self.seq_len = seq_len
#         self.pred_len = pred_len
#         self.num_vars = num_vars
#         self.patch_len = patch_len
#         self.stride = stride
        
#         self.revin = RevIN(num_vars)
        
#         # ËÆ°ÁÆóÂàáÁâá(Patch)ÁöÑÊï∞Èáè
#         self.num_patches = (seq_len - patch_len) // stride + 1
        
#         # Patch Êò†Â∞ÑÂ±Ç‰∏é‰ΩçÁΩÆÁºñÁ†Å
#         self.patch_embedding = nn.Linear(patch_len, d_model)
#         self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, d_model))
        
#         # Transformer ÁºñÁ†ÅÂô®
#         encoder_layer = nn.TransformerEncoderLayer(
#             d_model=d_model, 
#             nhead=n_heads, 
#             dim_feedforward=d_ff, 
#             dropout=dropout, 
#             batch_first=True, 
#             norm_first=True
#         )
#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)
        
#         # È¢ÑÊµãÂ§¥ (Â∞ÜÊâÄÊúâ Patch ÁöÑÁâπÂæÅÂ±ïÂπ≥ÂêéÊò†Â∞ÑÂà∞È¢ÑÊµãÈïøÂ∫¶)
#         self.head = nn.Linear(self.num_patches * d_model, pred_len)

#     def forward(self, x):
#         # x shape: [B, Seq_Len, Num_Vars] 
#         x = self.revin(x, 'norm')
#         B, L, N = x.shape
        
#         # üåü ÈáçÁÇπ 1ÔºöÈÄöÈÅìÁã¨Á´ã (Channel Independence)
#         # ÂΩ¢Áä∂Âèò‰∏∫ [B*N, Seq_Len]ÔºåÊØè‰∏™ÂèòÈáèË¢´ÂΩìÊàêÁã¨Á´ãÁöÑÊó∂Èó¥Â∫èÂàó
#         x = x.permute(0, 2, 1).reshape(B * N, L)
        
#         # üåü ÈáçÁÇπ 2ÔºöÂàáÁâá (Patching)
#         # ‰ΩøÁî® unfold ÊªëÂä®Á™óÂè£ÂàáÂâ≤Â∫èÂàó -> [B*N, num_patches, patch_len]
#         x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        
#         # ÂµåÂÖ•‰∏é‰ΩçÁΩÆÁºñÁ†Å: [B*N, num_patches, d_model]
#         x = self.patch_embedding(x) + self.position_embedding
        
#         # Transformer Â§ÑÁêÜ
#         x = self.encoder(x)
        
#         # Â±ïÂπ≥Âπ∂È¢ÑÊµãÊú™Êù•: [B*N, num_patches * d_model] -> [B*N, pred_len]
#         x = x.reshape(B * N, -1)
#         out = self.head(x) 
        
#         # ËøòÂéüÁª¥Â∫¶: [B*N, pred_len] -> [B, pred_len, Num_Vars]
#         out = out.view(B, N, self.pred_len).permute(0, 2, 1)
#         out = self.revin(out, 'denorm')
#         return out

# # ==========================================
# # 3. Êï∞ÊçÆÈõÜÂÆö‰πâ 
# # ==========================================
# class ElectricityDataset(Dataset):
#     def __init__(self, csv_path, flag='train'):
#         self.seq_len = 96; self.pred_len = 96
#         df_raw = pd.read_csv(csv_path)
#         df_data = df_raw.iloc[:, 1:] 
        
#         n = len(df_data)
#         num_train = int(n * 0.7); num_test = int(n * 0.2); num_val = n - num_train - num_test
        
#         border1s = [0, num_train - self.seq_len, n - num_test - self.seq_len]
#         border2s = [num_train, num_train + num_val, n]
        
#         f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
#         b1, b2 = border1s[f_idx], border2s[f_idx]
        
#         self.scaler = StandardScaler()
#         train_data = df_data.iloc[border1s[0]:border2s[0]]
#         self.scaler.fit(train_data.values)
#         data = self.scaler.transform(df_data.values)
        
#         self.data_x = data[b1:b2]
#         self.data_y = data[b1:b2]
    
#     def __getitem__(self, index):
#         s_begin = index; s_end = s_begin + self.seq_len
#         r_begin = s_end; r_end = r_begin + self.pred_len
#         return torch.tensor(self.data_x[s_begin:s_end], dtype=torch.float32), \
#                torch.tensor(self.data_y[r_begin:r_end], dtype=torch.float32)

#     def __len__(self): return len(self.data_x) - self.seq_len - self.pred_len + 1

# # ==========================================
# # 4. ‰∏ªËÆ≠ÁªÉÂæ™ÁéØ 
# # ==========================================
# def main():
#     if "LOCAL_RANK" in os.environ:
#         rank = int(os.environ["LOCAL_RANK"]); torch.cuda.set_device(rank)
#         dist.init_process_group("nccl"); world_size = dist.get_world_size()
#     else: rank = 0; world_size = 1; torch.cuda.set_device(0); dist.init_process_group("gloo", rank=0, world_size=1)

#     # Áã¨Á´ãÊó•ÂøóÊñá‰ª∂
#     log_file = "patchtst_log.csv"

#     dataset_path_file = ".kaggle_path.tmp"
#     if rank == 0:
#         print("üì• Initializing (Downloading dataset if needed)...")
#         path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
#         with open(dataset_path_file, "w") as f: f.write(path)
        
#         if not os.path.exists(log_file):
#             with open(log_file, "w") as f:
#                 f.write("Epoch,Train_HuberLoss,Val_MSE\n")
    
#     if dist.is_initialized(): dist.barrier()
    
#     with open(dataset_path_file, "r") as f:
#         base_path = f.read().strip()
#     csv_path = glob.glob(os.path.join(base_path, "**", "electricity.csv"), recursive=True)[0]

#     # PatchTST Ê≥®ÊÑèÂäõÁü©ÈòµÊûÅÂ∞èÔºåÈùûÂ∏∏ÁúÅÊòæÂ≠òÔºåBS ËÆæ‰∏∫ 64 (ÊØèÂç° 16) ÂÆåÂÖ®Ê≤°ÈóÆÈ¢ò
#     BS = 64 // world_size
#     train_ds = ElectricityDataset(csv_path, 'train')
#     val_ds = ElectricityDataset(csv_path, 'val')
    
#     train_loader = DataLoader(train_ds, batch_size=BS, sampler=DistributedSampler(train_ds) if world_size > 1 else None, shuffle=(world_size==1), num_workers=4)
#     val_loader = DataLoader(val_ds, batch_size=BS, sampler=DistributedSampler(val_ds, shuffle=False) if world_size > 1 else None, shuffle=False)

#     # ÂàùÂßãÂåñÂéüÁîü PatchTST
#     model = PatchTST(num_vars=321, d_model=128, n_heads=8, e_layers=3).to(rank)
#     if world_size > 1: model = DDP(model, device_ids=[rank])

#     optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30) 
    
#     # üåü ÂÆåÂÖ®ÂØπÈΩê‰Ω†ÁöÑËÆæÂÆöÔºöHuber Loss (delta=1.0) ËÆ≠ÁªÉ
#     criterion = nn.HuberLoss(delta=1.0) 

#     if rank == 0: print(f"üöÄ Pure PatchTST Started | Quiet Mode Active | HuberLoss(delta=1.0) | Epochs=30")
    
#     for epoch in range(30): 
#         if world_size > 1: train_loader.sampler.set_epoch(epoch)
#         model.train()
#         train_loss = 0
#         for bx, by in train_loader:
#             bx, by = bx.to(rank), by.to(rank)
#             optimizer.zero_grad(set_to_none=True)
#             with autocast(device_type='cuda', dtype=torch.bfloat16):
#                 pred = model(bx)
#                 loss = criterion(pred, by)
#             loss.backward()
#             optimizer.step()
#             train_loss += loss.item()
            
#         model.eval()
#         v_mse = 0; count = 0
#         with torch.no_grad():
#             for bx, by in val_loader:
#                 bx, by = bx.to(rank), by.to(rank)
#                 pred = model(bx)
#                 # üåü ÂÆåÂÖ®ÂØπÈΩêÔºöMSE È™åËØÅ
#                 v_mse += F.mse_loss(pred, by).item() 
#                 count += 1
                
#         avg_mse = torch.tensor(v_mse/count).to(rank)
#         if world_size > 1: dist.all_reduce(avg_mse, op=dist.ReduceOp.SUM); avg_mse /= world_size

#         if rank == 0:
#             avg_train_loss = train_loss/len(train_loader)
#             print(f"‚úÖ Epoch {epoch+1}/30 | Train HuberLoss: {avg_train_loss:.4f} | Val MSE: {avg_mse.item():.4f}")
            
#             with open(log_file, "a") as f:
#                 f.write(f"{epoch+1},{avg_train_loss:.4f},{avg_mse.item():.4f}\n")
                
#             torch.save(model.state_dict(), "patchtst_best.pth")
        
#         scheduler.step()

# if __name__ == "__main__": 
#     main()

import os, math, random, time, glob, torch, kagglehub
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
from sklearn.preprocessing import StandardScaler

# ==========================================
# 0. ÁéØÂ¢ÉÈÖçÁΩÆ‰∏éÂä†ÈÄü
# ==========================================
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# ==========================================
# 1. Ê†∏ÂøÉÁªÑ‰ª∂ÔºöRevIN 
# ==========================================
class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True):
        super(RevIN, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.affine = affine
        if self.affine: self._init_params()

    def _init_params(self):
        self.affine_weight = nn.Parameter(torch.ones(self.num_features))
        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

    def forward(self, x, mode:str):
        if mode == 'norm':
            self._get_statistics(x)
            x = self._normalize(x)
        elif mode == 'denorm':
            x = self._denormalize(x)
        return x

    def _get_statistics(self, x):
        dim2reduce = tuple(range(len(x.shape)-1))
        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()

    def _normalize(self, x):
        x = (x - self.mean) / self.stdev
        if self.affine: x = x * self.affine_weight + self.affine_bias
        return x

    def _denormalize(self, x):
        if self.affine:
            x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)
        x = x * self.stdev + self.mean
        return x

# ==========================================
# 2. ÂéüÁîü PatchTST Êû∂ÊûÑ
# ==========================================
class PatchTST(nn.Module):
    def __init__(self, seq_len=96, pred_len=96, num_vars=321, patch_len=16, stride=8, d_model=128, n_heads=8, e_layers=3, d_ff=512, dropout=0.1):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.num_vars = num_vars
        self.patch_len = patch_len
        self.stride = stride
        
        self.revin = RevIN(num_vars)
        
        # ËÆ°ÁÆóÂàáÁâá(Patch)ÁöÑÊï∞Èáè: ÂØπ‰∫é 96 ÈïøÂ∫¶Ôºåpatch=16, stride=8Ôºå‰ºöÂàáÂá∫ 11 ‰∏™ patch
        self.num_patches = (seq_len - patch_len) // stride + 1
        
        # Patch Êò†Â∞ÑÂ±Ç‰∏é‰ΩçÁΩÆÁºñÁ†Å
        self.patch_embedding = nn.Linear(patch_len, d_model)
        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, d_model))
        
        # Transformer ÁºñÁ†ÅÂô® (‰ªÖÂ§ÑÁêÜÊó∂Èó¥Áª¥Â∫¶ÔºåÈÄöÈÅìÂÆåÂÖ®Áã¨Á´ã)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=n_heads, 
            dim_feedforward=d_ff, 
            dropout=dropout, 
            batch_first=True, 
            norm_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)
        
        # È¢ÑÊµãÂ§¥ (Â∞ÜÊâÄÊúâ Patch ÁöÑÁâπÂæÅÂ±ïÂπ≥ÂêéÁõ¥Êé•Êò†Â∞ÑÂà∞Êú™Êù•ÈïøÂ∫¶)
        self.head = nn.Linear(self.num_patches * d_model, pred_len)

    def forward(self, x):
        # x shape: [B, Seq_Len, Num_Vars] 
        x = self.revin(x, 'norm')
        B, L, N = x.shape
        
        # 1. ÈÄöÈÅìÁã¨Á´ã (Channel Independence)
        # ÊääÊØè‰∏™ÂèòÈáèÂçïÁã¨ÊãéÂá∫Êù•: [B, L, N] -> [B, N, L] -> [B*N, L]
        x = x.permute(0, 2, 1).reshape(B * N, L)
        
        # 2. ÂàáÁâá (Patching)
        # Áî® unfold ÂàáÂâ≤: [B*N, num_patches, patch_len]
        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        
        # 3. ÂµåÂÖ•‰∏é‰ΩçÁΩÆÁºñÁ†Å: [B*N, num_patches, d_model]
        x = self.patch_embedding(x) + self.position_embedding
        
        # 4. Transformer Â§ÑÁêÜÊó∂Èó¥‰æùËµñ
        x = self.encoder(x)
        
        # 5. Â±ïÂπ≥Âπ∂È¢ÑÊµã: [B*N, num_patches * d_model] -> [B*N, pred_len]
        x = x.reshape(B * N, -1)
        out = self.head(x) 
        
        # 6. ËøòÂéüÁª¥Â∫¶Âπ∂ÂèçÂΩí‰∏ÄÂåñ: [B*N, pred_len] -> [B, pred_len, Num_Vars]
        out = out.view(B, N, self.pred_len).permute(0, 2, 1)
        out = self.revin(out, 'denorm')
        return out

# ==========================================
# 3. Êï∞ÊçÆÈõÜÂÆö‰πâ 
# ==========================================
class ElectricityDataset(Dataset):
    def __init__(self, csv_path, flag='train'):
        self.seq_len = 96; self.pred_len = 96
        df_raw = pd.read_csv(csv_path)
        df_data = df_raw.iloc[:, 1:] 
        
        n = len(df_data)
        num_train = int(n * 0.7); num_test = int(n * 0.2); num_val = n - num_train - num_test
        
        border1s = [0, num_train - self.seq_len, n - num_test - self.seq_len]
        border2s = [num_train, num_train + num_val, n]
        
        f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
        b1, b2 = border1s[f_idx], border2s[f_idx]
        
        self.scaler = StandardScaler()
        train_data = df_data.iloc[border1s[0]:border2s[0]]
        self.scaler.fit(train_data.values)
        data = self.scaler.transform(df_data.values)
        
        self.data_x = data[b1:b2]
        self.data_y = data[b1:b2]
    
    def __getitem__(self, index):
        s_begin = index; s_end = s_begin + self.seq_len
        r_begin = s_end; r_end = r_begin + self.pred_len
        return torch.tensor(self.data_x[s_begin:s_end], dtype=torch.float32), \
               torch.tensor(self.data_y[r_begin:r_end], dtype=torch.float32)

    def __len__(self): return len(self.data_x) - self.seq_len - self.pred_len + 1

# ==========================================
# 4. ‰∏ªËÆ≠ÁªÉÂæ™ÁéØ 
# ==========================================
def main():
    if "LOCAL_RANK" in os.environ:
        rank = int(os.environ["LOCAL_RANK"]); torch.cuda.set_device(rank)
        dist.init_process_group("nccl"); world_size = dist.get_world_size()
    else: rank = 0; world_size = 1; torch.cuda.set_device(0); dist.init_process_group("gloo", rank=0, world_size=1)

    log_file = "patchtst_mse_log.csv"

    dataset_path_file = ".kaggle_path.tmp"
    if rank == 0:
        print("üì• Initializing (Downloading dataset if needed)...")
        path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
        with open(dataset_path_file, "w") as f: f.write(path)
        
        if not os.path.exists(log_file):
            with open(log_file, "w") as f:
                # ËÆ∞ÂΩïÂ§¥ÊîπÂõû Train_MSE
                f.write("Epoch,Train_MSE,Val_MSE\n")
    
    if dist.is_initialized(): dist.barrier()
    
    with open(dataset_path_file, "r") as f:
        base_path = f.read().strip()
    csv_path = glob.glob(os.path.join(base_path, "**", "electricity.csv"), recursive=True)[0]

    BS = 64 // world_size
    train_ds = ElectricityDataset(csv_path, 'train')
    val_ds = ElectricityDataset(csv_path, 'val')
    
    train_loader = DataLoader(train_ds, batch_size=BS, sampler=DistributedSampler(train_ds) if world_size > 1 else None, shuffle=(world_size==1), num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=BS, sampler=DistributedSampler(val_ds, shuffle=False) if world_size > 1 else None, shuffle=False)

    model = PatchTST(num_vars=321, d_model=128, n_heads=8, e_layers=3).to(rank)
    if world_size > 1: model = DDP(model, device_ids=[rank])

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30) 
    
    # üåü ÈáçÁÇπÊõøÊç¢ÔºöËÆ≠ÁªÉ‰πü‰ΩøÁî®ÂéüÁîü MSE Loss
    criterion = nn.MSELoss() 

    if rank == 0: print(f"üöÄ Native PatchTST Started | Quiet Mode Active | MSE Loss Only | Epochs=30")
    
    for epoch in range(30): 
        if world_size > 1: train_loader.sampler.set_epoch(epoch)
        model.train()
        train_loss = 0
        for bx, by in train_loader:
            bx, by = bx.to(rank), by.to(rank)
            optimizer.zero_grad(set_to_none=True)
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                pred = model(bx)
                loss = criterion(pred, by) # ‰ΩøÁî® MSE ËÆ°ÁÆóÊ¢ØÂ∫¶
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        model.eval()
        v_mse = 0; count = 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(rank), by.to(rank)
                pred = model(bx)
                v_mse += F.mse_loss(pred, by).item() 
                count += 1
                
        avg_mse = torch.tensor(v_mse/count).to(rank)
        if world_size > 1: dist.all_reduce(avg_mse, op=dist.ReduceOp.SUM); avg_mse /= world_size

        if rank == 0:
            avg_train_loss = train_loss/len(train_loader)
            print(f"‚úÖ Epoch {epoch+1}/30 | Train MSE: {avg_train_loss:.4f} | Val MSE: {avg_mse.item():.4f}")
            
            with open(log_file, "a") as f:
                f.write(f"{epoch+1},{avg_train_loss:.4f},{avg_mse.item():.4f}\n")
                
            torch.save(model.state_dict(), "patchtst_native_best.pth")
        
        scheduler.step()

if __name__ == "__main__": main()



ÂÆòÊñπpatchtstÔºå È°∫Â∫èÔºölog,‰ª£Á†Å„ÄÇ ÂèØ‰ª•ÁúãÂà∞ËøôÁâàÂÆòÊñπÂ§çÁé∞Âá∫Êù•:üèÜ Final Test MSE: 0.18640, Áî®ÁöÑÂÆòÊñπÁöÑ‰ª£Á†ÅÔºåÊ≤°ÊúâgeminiËá™Â∑±ÂÜôÁöÑpatchtstÊïàÊûúÂ•Ω

(base) root@ubuntu22:~# python -m torch.distributed.run --nproc_per_node=4 test.py
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
üî• Official PatchTST DDP Training Start!
Epoch [01/30] | Val MSE: 0.20096
Epoch [02/30] | Val MSE: 0.18715
Epoch [03/30] | Val MSE: 0.18274
Epoch [04/30] | Val MSE: 0.17975
Epoch [05/30] | Val MSE: 0.17714
Epoch [06/30] | Val MSE: 0.17597
Epoch [07/30] | Val MSE: 0.17537
Epoch [08/30] | Val MSE: 0.17347
Epoch [09/30] | Val MSE: 0.17355
Epoch [10/30] | Val MSE: 0.17126
Epoch [11/30] | Val MSE: 0.17177
Epoch [12/30] | Val MSE: 0.17098
Epoch [13/30] | Val MSE: 0.17023
Epoch [14/30] | Val MSE: 0.16927
Epoch [15/30] | Val MSE: 0.16944
Epoch [16/30] | Val MSE: 0.16847
Epoch [17/30] | Val MSE: 0.16818
Epoch [18/30] | Val MSE: 0.16803
Epoch [19/30] | Val MSE: 0.16856
Epoch [20/30] | Val MSE: 0.16757
Epoch [21/30] | Val MSE: 0.16743
Epoch [22/30] | Val MSE: 0.16691
Epoch [23/30] | Val MSE: 0.16787
Epoch [24/30] | Val MSE: 0.16647
Epoch [25/30] | Val MSE: 0.16803
Epoch [26/30] | Val MSE: 0.16570
Epoch [27/30] | Val MSE: 0.16584
Epoch [28/30] | Val MSE: 0.16589
Epoch [29/30] | Val MSE: 0.16516
Epoch [30/30] | Val MSE: 0.16483

üèÅ Training Finished. Evaluating on Test Set...
üèÜ Final Test MSE: 0.18640 | Test MAE: 0.28000
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import torch.nn.functional as F
# from torch.utils.data import DataLoader, Dataset
# from torch.utils.data.distributed import DistributedSampler
# import torch.distributed as dist
# from torch.nn.parallel import DistributedDataParallel as DDP
# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import StandardScaler
# import os
# import glob
# import kagglehub
# import math

# # ==============================================================================
# # üõ†Ô∏è ÂÆòÊñπÂ∫ïÂ±ÇÁªÑ‰ª∂ (1:1 ËøòÂéü)
# # ==============================================================================

# class Transpose(nn.Module):
#     def __init__(self, *dims, contiguous=False):
#         super().__init__()
#         self.dims, self.contiguous = dims, contiguous
#     def forward(self, x):
#         if self.contiguous: return x.transpose(*self.dims).contiguous()
#         return x.transpose(*self.dims)

# class PatchEmbedding(nn.Module):
#     def __init__(self, d_model, patch_len, stride, padding, dropout):
#         super().__init__()
#         self.patch_len = patch_len
#         self.stride = stride
#         self.padding_patch = nn.ReplicationPad1d((0, padding))
#         self.value_embedding = nn.Linear(patch_len, d_model)
#         self.dropout = nn.Dropout(dropout)

#     def forward(self, x):
#         n_vars = x.shape[1]
#         x = self.padding_patch(x)
#         x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
#         x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))
#         x = self.value_embedding(x)
#         return x, n_vars

# class FullAttention(nn.Module):
#     def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):
#         super(FullAttention, self).__init__()
#         self.scale = scale
#         self.mask_flag = mask_flag
#         self.output_attention = output_attention
#         self.dropout = nn.Dropout(attention_dropout)

#     def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):
#         B, L, H, E = queries.shape
#         _, S, _, D = keys.shape
#         scale = self.scale or 1. / math.sqrt(E)
#         scores = torch.einsum("blhe,bshe->blhs", queries, keys)
#         if self.mask_flag and attn_mask is not None:
#             scores.masked_fill_(attn_mask.mask, -np.inf)
#         A = self.dropout(torch.softmax(scale * scores, dim=-1))
#         V = torch.einsum("blhs,bshe->blhe", A, values)
#         return (V.contiguous(), A) if self.output_attention else (V.contiguous(), None)

# class AttentionLayer(nn.Module):
#     def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):
#         super(AttentionLayer, self).__init__()
#         d_keys = d_keys or (d_model // n_heads)
#         d_values = d_values or (d_model // n_heads)
#         self.inner_attention = attention
#         self.query_projection = nn.Linear(d_model, d_keys * n_heads)
#         self.key_projection = nn.Linear(d_model, d_keys * n_heads)
#         self.value_projection = nn.Linear(d_model, d_values * n_heads)
#         self.out_projection = nn.Linear(d_values * n_heads, d_model)
#         self.n_heads = n_heads

#     def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):
#         B, L, _ = queries.shape; H = self.n_heads
#         queries = self.query_projection(queries).view(B, L, H, -1)
#         keys = self.key_projection(keys).view(B, L, H, -1)
#         values = self.value_projection(values).view(B, L, H, -1)
#         out, attn = self.inner_attention(queries, keys, values, attn_mask, tau, delta)
#         return self.out_projection(out.view(B, L, -1)), attn

# class EncoderLayer(nn.Module):
#     def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation="relu"):
#         super(EncoderLayer, self).__init__()
#         d_ff = d_ff or 4 * d_model
#         self.attention = attention
#         self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
#         self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
#         self.norm1 = nn.LayerNorm(d_model); self.norm2 = nn.LayerNorm(d_model)
#         self.dropout = nn.Dropout(dropout)
#         self.activation = F.relu if activation == "relu" else F.gelu

#     def forward(self, x, attn_mask=None, tau=None, delta=None):
#         new_x, attn = self.attention(x, x, x, attn_mask=attn_mask, tau=tau, delta=delta)
#         x = self.norm1(x + self.dropout(new_x))
#         y = self.dropout(self.activation(self.conv1(x.transpose(-1, 1))))
#         y = self.dropout(self.conv2(y).transpose(-1, 1))
#         return self.norm2(x + y), attn

# class Encoder(nn.Module):
#     def __init__(self, attn_layers, norm_layer=None):
#         super(Encoder, self).__init__()
#         self.attn_layers = nn.ModuleList(attn_layers)
#         self.norm = norm_layer
#     def forward(self, x, attn_mask=None, tau=None, delta=None):
#         attns = []
#         for attn_layer in self.attn_layers:
#             x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
#             attns.append(attn)
#         if self.norm is not None: x = self.norm(x)
#         return x, attns

# class FlattenHead(nn.Module):
#     def __init__(self, n_vars, nf, target_window, head_dropout=0):
#         super().__init__()
#         self.flatten = nn.Flatten(start_dim=-2)
#         self.linear = nn.Linear(nf, target_window)
#         self.dropout = nn.Dropout(head_dropout)
#     def forward(self, x):
#         return self.dropout(self.linear(self.flatten(x)))

# # ==============================================================================
# # üèÜ Ê†∏ÂøÉ‰∏ªÁ±ª: Model (PatchTST)
# # ==============================================================================

# class Model(nn.Module):
#     def __init__(self, configs, patch_len=16, stride=8):
#         super().__init__()
#         self.seq_len = configs.seq_len; self.pred_len = configs.pred_len
#         padding = stride
#         self.patch_embedding = PatchEmbedding(configs.d_model, patch_len, stride, padding, configs.dropout)
#         self.encoder = Encoder(
#             [EncoderLayer(AttentionLayer(FullAttention(False, configs.factor, attention_dropout=configs.dropout), 
#              configs.d_model, configs.n_heads), configs.d_model, configs.d_ff, dropout=configs.dropout, 
#              activation=configs.activation) for l in range(configs.e_layers)],
#             norm_layer=nn.Sequential(Transpose(1,2), nn.BatchNorm1d(configs.d_model), Transpose(1,2))
#         )
#         self.head_nf = configs.d_model * int((configs.seq_len - patch_len) / stride + 2)
#         self.head = FlattenHead(configs.enc_in, self.head_nf, configs.pred_len, head_dropout=configs.dropout)
#         self.position_embedding = nn.Parameter(torch.randn(1, int((configs.seq_len - patch_len) / stride + 2), configs.d_model))

#     def forecast(self, x_enc):
#         # üåü RevIN ÂΩí‰∏ÄÂåñ
#         means = x_enc.mean(1, keepdim=True).detach()
#         x_enc = x_enc - means
#         stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
#         x_enc /= stdev

#         x_enc = x_enc.permute(0, 2, 1)
#         enc_out, n_vars = self.patch_embedding(x_enc)
#         enc_out = enc_out + self.position_embedding
#         enc_out, _ = self.encoder(enc_out)
        
#         enc_out = torch.reshape(enc_out, (-1, n_vars, enc_out.shape[-2], enc_out.shape[-1]))
#         enc_out = enc_out.permute(0, 1, 3, 2)
#         dec_out = self.head(enc_out).permute(0, 2, 1)

#         # üåü RevIN ÂèçÂΩí‰∏ÄÂåñ
#         dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
#         dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
#         return dec_out

#     def forward(self, x_enc, x_mark_enc=None, x_dec=None, x_mark_dec=None, mask=None):
#         return self.forecast(x_enc)

# # ==============================================================================
# # üöÄ ËøêË°åÂºïÊìé (DDP + Test MSE & MAE)
# # ==============================================================================

# class ElectricityDataset(Dataset):
#     def __init__(self, csv_path, flag='train', seq_len=96, pred_len=96):
#         df_raw = pd.read_csv(csv_path)
#         data = df_raw.values[:, 1:].astype(np.float32)
#         num_train = int(len(data) * 0.7); num_test = int(len(data) * 0.2)
#         border1s = [0, num_train - seq_len, len(data) - num_test - seq_len]
#         border2s = [num_train, num_train + (len(data)-num_train-num_test), len(data)]
#         idx = {'train': 0, 'val': 1, 'test': 2}[flag]
#         self.scaler = StandardScaler(); self.scaler.fit(data[border1s[0]:border2s[0]])
#         self.data = self.scaler.transform(data[border1s[idx]:border2s[idx]])
#         self.seq_len, self.pred_len = seq_len, pred_len
#     def __getitem__(self, index):
#         return torch.tensor(self.data[index:index+self.seq_len]), \
#                torch.tensor(self.data[index+self.seq_len:index+self.seq_len+self.pred_len])
#     def __len__(self): return len(self.data) - self.seq_len - self.pred_len + 1

# class Configs:
#     task_name = 'long_term_forecast'; seq_len = 96; pred_len = 96; d_model = 128
#     n_heads = 8; e_layers = 3; d_ff = 512; dropout = 0.1; activation = 'gelu'
#     factor = 3; enc_in = 321

# def run_training():
#     dist.init_process_group(backend='nccl')
#     local_rank = int(os.environ["LOCAL_RANK"]); torch.cuda.set_device(local_rank)
#     device = torch.device("cuda", local_rank); torch.cuda.empty_cache()

#     if local_rank == 0:
#         path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
#         with open(".kaggle_path.tmp", "w") as f: f.write(path)
#     dist.barrier()
#     with open(".kaggle_path.tmp", "r") as f: base_path = f.read().strip()
#     csv_path = glob.glob(os.path.join(base_path, "**", "electricity.csv"), recursive=True)[0]

#     model = Model(Configs()).to(device)
#     model = DDP(model, device_ids=[local_rank], output_device=local_rank)

#     train_ds = ElectricityDataset(csv_path, 'train')
#     train_loader = DataLoader(train_ds, batch_size=32, sampler=DistributedSampler(train_ds), num_workers=2)
#     val_loader = DataLoader(ElectricityDataset(csv_path, 'val'), batch_size=32, shuffle=False)
#     test_loader = DataLoader(ElectricityDataset(csv_path, 'test'), batch_size=32, shuffle=False)

#     optimizer = optim.Adam(model.parameters(), lr=1e-4)
#     criterion_mse = nn.MSELoss()
#     criterion_mae = nn.L1Loss()  # üåü Â¢ûÂä† MAE ÊçüÂ§±ÂáΩÊï∞Áî®‰∫éËØÑ‰º∞
#     best_val_mse = float('inf')

#     if local_rank == 0: print("üî• Official PatchTST DDP Training Start!")

#     for epoch in range(1, 31):
#         train_loader.sampler.set_epoch(epoch); model.train()
#         for x, y in train_loader:
#             optimizer.zero_grad()
#             out = model(x.to(device, non_blocking=True))
#             loss = criterion_mse(out, y.to(device, non_blocking=True))
#             loss.backward(); optimizer.step()

#         if local_rank == 0:
#             model.eval(); v_mse = 0
#             with torch.no_grad():
#                 for x, y in val_loader:
#                     out = model(x.to(device, non_blocking=True))
#                     v_mse += criterion_mse(out, y.to(device, non_blocking=True)).item()
#             avg_v_mse = v_mse/len(val_loader)
#             print(f"Epoch [{epoch:02d}/30] | Val MSE: {avg_v_mse:.5f}")
#             if avg_v_mse < best_val_mse:
#                 best_val_mse = avg_v_mse
#                 torch.save(model.module.state_dict(), "best_official_patchtst.pth")

#     # üåü Final Test Phase (‰∏ªËøõÁ®ãÁã¨‰∫´ÊµãËØïËÆ°ÁÆó)
#     dist.barrier()
#     if local_rank == 0:
#         print("\nüèÅ Training Finished. Evaluating on Test Set...")
#         # Ëá™Âä®Âä†ËΩΩ Val MSE ÊúÄ‰ΩéÁöÑÊ®°ÂûãÊùÉÈáç
#         model.module.load_state_dict(torch.load("best_official_patchtst.pth"))
#         model.eval()
#         t_mse = 0
#         t_mae = 0
#         with torch.no_grad():
#             for x, y in test_loader:
#                 out = model(x.to(device, non_blocking=True))
#                 # Á¥ØÂä† MSE Âíå MAE
#                 t_mse += criterion_mse(out, y.to(device, non_blocking=True)).item()
#                 t_mae += criterion_mae(out, y.to(device, non_blocking=True)).item()
                
#         # ÊâìÂç∞ÊúÄÁªàÊâìÊ¶úÊàêÁª©
#         print(f"üèÜ Final Test MSE: {t_mse/len(test_loader):.5f} | Test MAE: {t_mae/len(test_loader):.5f}")

#     dist.destroy_process_group()

# if __name__ == "__main__":
#     run_training()


