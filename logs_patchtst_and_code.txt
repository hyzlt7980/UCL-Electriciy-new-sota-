
ÂÆòÊñπpatchtstÔºå È°∫Â∫èÔºölog,‰ª£Á†Å„ÄÇ ÂèØ‰ª•ÁúãÂà∞ËøôÁâàÂÆòÊñπÂ§çÁé∞Âá∫Êù•:üèÜ Final Test MSE: 0.18640, Áî®ÁöÑÂÆòÊñπÁöÑ‰ª£Á†ÅÔºågeminiÂÅö‰∫ÜÁÆÄÂçï‰øÆÊîπ
(base) root@ubuntu22:~# python -m torch.distributed.run --nproc_per_node=4 test.py
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
üî• Official PatchTST DDP Training Start!
Epoch [01/30] | Val MSE: 0.20096
Epoch [02/30] | Val MSE: 0.18715
Epoch [03/30] | Val MSE: 0.18274
Epoch [04/30] | Val MSE: 0.17975
Epoch [05/30] | Val MSE: 0.17714
Epoch [06/30] | Val MSE: 0.17597
Epoch [07/30] | Val MSE: 0.17537
Epoch [08/30] | Val MSE: 0.17347
Epoch [09/30] | Val MSE: 0.17355
Epoch [10/30] | Val MSE: 0.17126
Epoch [11/30] | Val MSE: 0.17177
Epoch [12/30] | Val MSE: 0.17098
Epoch [13/30] | Val MSE: 0.17023
Epoch [14/30] | Val MSE: 0.16927
Epoch [15/30] | Val MSE: 0.16944
Epoch [16/30] | Val MSE: 0.16847
Epoch [17/30] | Val MSE: 0.16818
Epoch [18/30] | Val MSE: 0.16803
Epoch [19/30] | Val MSE: 0.16856
Epoch [20/30] | Val MSE: 0.16757
Epoch [21/30] | Val MSE: 0.16743
Epoch [22/30] | Val MSE: 0.16691
Epoch [23/30] | Val MSE: 0.16787
Epoch [24/30] | Val MSE: 0.16647
Epoch [25/30] | Val MSE: 0.16803
Epoch [26/30] | Val MSE: 0.16570
Epoch [27/30] | Val MSE: 0.16584
Epoch [28/30] | Val MSE: 0.16589
Epoch [29/30] | Val MSE: 0.16516
Epoch [30/30] | Val MSE: 0.16483

üèÅ Training Finished. Evaluating on Test Set...
üèÜ Final Test MSE: 0.18640 | Test MAE: 0.28000
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import torch.nn.functional as F
# from torch.utils.data import DataLoader, Dataset
# from torch.utils.data.distributed import DistributedSampler
# import torch.distributed as dist
# from torch.nn.parallel import DistributedDataParallel as DDP
# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import StandardScaler
# import os
# import glob
# import kagglehub
# import math

# # ==============================================================================
# # üõ†Ô∏è ÂÆòÊñπÂ∫ïÂ±ÇÁªÑ‰ª∂ (1:1 ËøòÂéü)
# # ==============================================================================

# class Transpose(nn.Module):
#     def __init__(self, *dims, contiguous=False):
#         super().__init__()
#         self.dims, self.contiguous = dims, contiguous
#     def forward(self, x):
#         if self.contiguous: return x.transpose(*self.dims).contiguous()
#         return x.transpose(*self.dims)

# class PatchEmbedding(nn.Module):
#     def __init__(self, d_model, patch_len, stride, padding, dropout):
#         super().__init__()
#         self.patch_len = patch_len
#         self.stride = stride
#         self.padding_patch = nn.ReplicationPad1d((0, padding))
#         self.value_embedding = nn.Linear(patch_len, d_model)
#         self.dropout = nn.Dropout(dropout)

#     def forward(self, x):
#         n_vars = x.shape[1]
#         x = self.padding_patch(x)
#         x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
#         x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))
#         x = self.value_embedding(x)
#         return x, n_vars

# class FullAttention(nn.Module):
#     def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):
#         super(FullAttention, self).__init__()
#         self.scale = scale
#         self.mask_flag = mask_flag
#         self.output_attention = output_attention
#         self.dropout = nn.Dropout(attention_dropout)

#     def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):
#         B, L, H, E = queries.shape
#         _, S, _, D = keys.shape
#         scale = self.scale or 1. / math.sqrt(E)
#         scores = torch.einsum("blhe,bshe->blhs", queries, keys)
#         if self.mask_flag and attn_mask is not None:
#             scores.masked_fill_(attn_mask.mask, -np.inf)
#         A = self.dropout(torch.softmax(scale * scores, dim=-1))
#         V = torch.einsum("blhs,bshe->blhe", A, values)
#         return (V.contiguous(), A) if self.output_attention else (V.contiguous(), None)

# class AttentionLayer(nn.Module):
#     def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):
#         super(AttentionLayer, self).__init__()
#         d_keys = d_keys or (d_model // n_heads)
#         d_values = d_values or (d_model // n_heads)
#         self.inner_attention = attention
#         self.query_projection = nn.Linear(d_model, d_keys * n_heads)
#         self.key_projection = nn.Linear(d_model, d_keys * n_heads)
#         self.value_projection = nn.Linear(d_model, d_values * n_heads)
#         self.out_projection = nn.Linear(d_values * n_heads, d_model)
#         self.n_heads = n_heads

#     def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):
#         B, L, _ = queries.shape; H = self.n_heads
#         queries = self.query_projection(queries).view(B, L, H, -1)
#         keys = self.key_projection(keys).view(B, L, H, -1)
#         values = self.value_projection(values).view(B, L, H, -1)
#         out, attn = self.inner_attention(queries, keys, values, attn_mask, tau, delta)
#         return self.out_projection(out.view(B, L, -1)), attn

# class EncoderLayer(nn.Module):
#     def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation="relu"):
#         super(EncoderLayer, self).__init__()
#         d_ff = d_ff or 4 * d_model
#         self.attention = attention
#         self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
#         self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
#         self.norm1 = nn.LayerNorm(d_model); self.norm2 = nn.LayerNorm(d_model)
#         self.dropout = nn.Dropout(dropout)
#         self.activation = F.relu if activation == "relu" else F.gelu

#     def forward(self, x, attn_mask=None, tau=None, delta=None):
#         new_x, attn = self.attention(x, x, x, attn_mask=attn_mask, tau=tau, delta=delta)
#         x = self.norm1(x + self.dropout(new_x))
#         y = self.dropout(self.activation(self.conv1(x.transpose(-1, 1))))
#         y = self.dropout(self.conv2(y).transpose(-1, 1))
#         return self.norm2(x + y), attn

# class Encoder(nn.Module):
#     def __init__(self, attn_layers, norm_layer=None):
#         super(Encoder, self).__init__()
#         self.attn_layers = nn.ModuleList(attn_layers)
#         self.norm = norm_layer
#     def forward(self, x, attn_mask=None, tau=None, delta=None):
#         attns = []
#         for attn_layer in self.attn_layers:
#             x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
#             attns.append(attn)
#         if self.norm is not None: x = self.norm(x)
#         return x, attns

# class FlattenHead(nn.Module):
#     def __init__(self, n_vars, nf, target_window, head_dropout=0):
#         super().__init__()
#         self.flatten = nn.Flatten(start_dim=-2)
#         self.linear = nn.Linear(nf, target_window)
#         self.dropout = nn.Dropout(head_dropout)
#     def forward(self, x):
#         return self.dropout(self.linear(self.flatten(x)))

# # ==============================================================================
# # üèÜ Ê†∏ÂøÉ‰∏ªÁ±ª: Model (PatchTST)
# # ==============================================================================

# class Model(nn.Module):
#     def __init__(self, configs, patch_len=16, stride=8):
#         super().__init__()
#         self.seq_len = configs.seq_len; self.pred_len = configs.pred_len
#         padding = stride
#         self.patch_embedding = PatchEmbedding(configs.d_model, patch_len, stride, padding, configs.dropout)
#         self.encoder = Encoder(
#             [EncoderLayer(AttentionLayer(FullAttention(False, configs.factor, attention_dropout=configs.dropout), 
#              configs.d_model, configs.n_heads), configs.d_model, configs.d_ff, dropout=configs.dropout, 
#              activation=configs.activation) for l in range(configs.e_layers)],
#             norm_layer=nn.Sequential(Transpose(1,2), nn.BatchNorm1d(configs.d_model), Transpose(1,2))
#         )
#         self.head_nf = configs.d_model * int((configs.seq_len - patch_len) / stride + 2)
#         self.head = FlattenHead(configs.enc_in, self.head_nf, configs.pred_len, head_dropout=configs.dropout)
#         self.position_embedding = nn.Parameter(torch.randn(1, int((configs.seq_len - patch_len) / stride + 2), configs.d_model))

#     def forecast(self, x_enc):
#         # üåü RevIN ÂΩí‰∏ÄÂåñ
#         means = x_enc.mean(1, keepdim=True).detach()
#         x_enc = x_enc - means
#         stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
#         x_enc /= stdev

#         x_enc = x_enc.permute(0, 2, 1)
#         enc_out, n_vars = self.patch_embedding(x_enc)
#         enc_out = enc_out + self.position_embedding
#         enc_out, _ = self.encoder(enc_out)
        
#         enc_out = torch.reshape(enc_out, (-1, n_vars, enc_out.shape[-2], enc_out.shape[-1]))
#         enc_out = enc_out.permute(0, 1, 3, 2)
#         dec_out = self.head(enc_out).permute(0, 2, 1)

#         # üåü RevIN ÂèçÂΩí‰∏ÄÂåñ
#         dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
#         dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
#         return dec_out

#     def forward(self, x_enc, x_mark_enc=None, x_dec=None, x_mark_dec=None, mask=None):
#         return self.forecast(x_enc)

# # ==============================================================================
# # üöÄ ËøêË°åÂºïÊìé (DDP + Test MSE & MAE)
# # ==============================================================================

# class ElectricityDataset(Dataset):
#     def __init__(self, csv_path, flag='train', seq_len=96, pred_len=96):
#         df_raw = pd.read_csv(csv_path)
#         data = df_raw.values[:, 1:].astype(np.float32)
#         num_train = int(len(data) * 0.7); num_test = int(len(data) * 0.2)
#         border1s = [0, num_train - seq_len, len(data) - num_test - seq_len]
#         border2s = [num_train, num_train + (len(data)-num_train-num_test), len(data)]
#         idx = {'train': 0, 'val': 1, 'test': 2}[flag]
#         self.scaler = StandardScaler(); self.scaler.fit(data[border1s[0]:border2s[0]])
#         self.data = self.scaler.transform(data[border1s[idx]:border2s[idx]])
#         self.seq_len, self.pred_len = seq_len, pred_len
#     def __getitem__(self, index):
#         return torch.tensor(self.data[index:index+self.seq_len]), \
#                torch.tensor(self.data[index+self.seq_len:index+self.seq_len+self.pred_len])
#     def __len__(self): return len(self.data) - self.seq_len - self.pred_len + 1

# class Configs:
#     task_name = 'long_term_forecast'; seq_len = 96; pred_len = 96; d_model = 128
#     n_heads = 8; e_layers = 3; d_ff = 512; dropout = 0.1; activation = 'gelu'
#     factor = 3; enc_in = 321

# def run_training():
#     dist.init_process_group(backend='nccl')
#     local_rank = int(os.environ["LOCAL_RANK"]); torch.cuda.set_device(local_rank)
#     device = torch.device("cuda", local_rank); torch.cuda.empty_cache()

#     if local_rank == 0:
#         path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
#         with open(".kaggle_path.tmp", "w") as f: f.write(path)
#     dist.barrier()
#     with open(".kaggle_path.tmp", "r") as f: base_path = f.read().strip()
#     csv_path = glob.glob(os.path.join(base_path, "**", "electricity.csv"), recursive=True)[0]

#     model = Model(Configs()).to(device)
#     model = DDP(model, device_ids=[local_rank], output_device=local_rank)

#     train_ds = ElectricityDataset(csv_path, 'train')
#     train_loader = DataLoader(train_ds, batch_size=32, sampler=DistributedSampler(train_ds), num_workers=2)
#     val_loader = DataLoader(ElectricityDataset(csv_path, 'val'), batch_size=32, shuffle=False)
#     test_loader = DataLoader(ElectricityDataset(csv_path, 'test'), batch_size=32, shuffle=False)

#     optimizer = optim.Adam(model.parameters(), lr=1e-4)
#     criterion_mse = nn.MSELoss()
#     criterion_mae = nn.L1Loss()  # üåü Â¢ûÂä† MAE ÊçüÂ§±ÂáΩÊï∞Áî®‰∫éËØÑ‰º∞
#     best_val_mse = float('inf')

#     if local_rank == 0: print("üî• Official PatchTST DDP Training Start!")

#     for epoch in range(1, 31):
#         train_loader.sampler.set_epoch(epoch); model.train()
#         for x, y in train_loader:
#             optimizer.zero_grad()
#             out = model(x.to(device, non_blocking=True))
#             loss = criterion_mse(out, y.to(device, non_blocking=True))
#             loss.backward(); optimizer.step()

#         if local_rank == 0:
#             model.eval(); v_mse = 0
#             with torch.no_grad():
#                 for x, y in val_loader:
#                     out = model(x.to(device, non_blocking=True))
#                     v_mse += criterion_mse(out, y.to(device, non_blocking=True)).item()
#             avg_v_mse = v_mse/len(val_loader)
#             print(f"Epoch [{epoch:02d}/30] | Val MSE: {avg_v_mse:.5f}")
#             if avg_v_mse < best_val_mse:
#                 best_val_mse = avg_v_mse
#                 torch.save(model.module.state_dict(), "best_official_patchtst.pth")

#     # üåü Final Test Phase (‰∏ªËøõÁ®ãÁã¨‰∫´ÊµãËØïËÆ°ÁÆó)
#     dist.barrier()
#     if local_rank == 0:
#         print("\nüèÅ Training Finished. Evaluating on Test Set...")
#         # Ëá™Âä®Âä†ËΩΩ Val MSE ÊúÄ‰ΩéÁöÑÊ®°ÂûãÊùÉÈáç
#         model.module.load_state_dict(torch.load("best_official_patchtst.pth"))
#         model.eval()
#         t_mse = 0
#         t_mae = 0
#         with torch.no_grad():
#             for x, y in test_loader:
#                 out = model(x.to(device, non_blocking=True))
#                 # Á¥ØÂä† MSE Âíå MAE
#                 t_mse += criterion_mse(out, y.to(device, non_blocking=True)).item()
#                 t_mae += criterion_mae(out, y.to(device, non_blocking=True)).item()
                
#         # ÊâìÂç∞ÊúÄÁªàÊâìÊ¶úÊàêÁª©
#         print(f"üèÜ Final Test MSE: {t_mse/len(test_loader):.5f} | Test MAE: {t_mae/len(test_loader):.5f}")

#     dist.destroy_process_group()

# if __name__ == "__main__":
#     run_training()









