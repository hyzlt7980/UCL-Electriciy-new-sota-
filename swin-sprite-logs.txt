åˆ—0  åˆ—1  åˆ—2  åˆ—3 | åˆ—4  åˆ—5  åˆ—6  åˆ—7 | åˆ—8  åˆ—9 åˆ—10 åˆ—11
      +-------------------+-------------------+-------------------+
è¡Œ 0  |  0    1    2    3 |  4    5    6    7 |  8    9   10   11 |
è¡Œ 1  | 12   13   14   15 | 16   17   18   19 | 20   21   22   23 |
è¡Œ 2  | 24   25   26   27 | 28   29   30   31 | 32   33   34   35 |
è¡Œ 3  | 36   37   38   39 | 40   41   42   43 | 44   45   46   47 |
      +-------------------+-------------------+-------------------+
è¡Œ 4  | 48   49   50   51 | 52   53   54   55 | 56   57   58   59 |
è¡Œ 5  | 60   61   62   63 | 64   65   66   67 | 68   69   70   71 |
è¡Œ 6  | 72   73   74   75 | 76   77   78   79 | 80   81   82   83 |
è¡Œ 7  | 84   85   86   87 | 88   89   90   91 | 92   93   94   95 |
      +-------------------+-------------------+-------------------+



#å¸¦å™ªå£°å»å™ªçš„swinifold,
import os, math, random, time, glob, torch, kagglehub
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
from sklearn.preprocessing import StandardScaler
from timm.models.swin_transformer import SwinTransformerBlock
from timm.layers import Mlp, LayerNorm

# ==========================================
# 0. åŠ é€Ÿä¸ç¯å¢ƒé…ç½®
# ==========================================
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# ==========================================
# 1. æ ¸å¿ƒç²¾è°ƒç»„ä»¶ï¼šDAE é‡å»ºå— (ä½ æä¾›çš„é€»è¾‘)
# ==========================================
class DenoisingDisruptionBlock(nn.Module):
    def __init__(self, dim, num_heads=8, noise_level=0.1): 
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = Mlp(in_features=dim, hidden_features=dim*4, out_features=dim)
        self.noise_level = noise_level
        self.denoise_loss_fn = nn.MSELoss()
        self.denoise_proj = nn.Linear(dim, dim)
        
    def forward(self, x):
        x_clean = x
        if self.training: 
            # ä¸»åŠ¨ç ´åï¼šæ³¨å…¥ä½ è¦æ±‚çš„å™ªå£° Level
            x_noisy = x + torch.randn_like(x) * self.noise_level
        else:
            x_noisy = x 
        
        attn_out, _ = self.attn(self.norm1(x_noisy), self.norm1(x_noisy), self.norm1(x_noisy))
        x_p = x_noisy + attn_out
        x_p = x_p + self.mlp(self.norm2(x_p))
        
        if self.training: 
            # è®¡ç®—é‡å»ºæŸå¤±ï¼Œç”¨äºè¾…åŠ©ç›‘ç£ç‰¹å¾è´¨é‡
            d_loss = self.denoise_loss_fn(self.denoise_proj(x_p), x_clean.detach())
            return x_p, d_loss
        return x_p, torch.tensor(0.0, device=x.device)

# ==========================================
# 2. å±‚æ¬¡åŒ–é›†æˆæ¶æ„ (Level 1 å¢å¼º)
# ==========================================
class Swin_iFold_V6(nn.Module):
    def __init__(self, num_vars=321, embed_dim=128):
        super().__init__()
        self.H, self.W = 8, 12 
        self.revin = RevIN(num_vars)
        self.patch_embed = nn.Linear(1, embed_dim)
        
        # --- Level 1: åŸå§‹ç½‘æ ¼ (8x12) ---
        # é€»è¾‘ï¼šSwin1 -> Noise -> Swin2 -> Denoise(DAE)
        self.l1_swin1 = SwinTransformerBlock(dim=embed_dim, input_resolution=(8, 12), num_heads=8, window_size=4, shift_size=0)
        self.l1_swin2 = SwinTransformerBlock(dim=embed_dim, input_resolution=(8, 12), num_heads=8, window_size=4, shift_size=2)
        self.dae_block = DenoisingDisruptionBlock(embed_dim, num_heads=8, noise_level=0.1)
        
        # ä¸‹é‡‡æ ·ï¼š8x12 -> 4x6
        self.reduction = nn.Conv2d(embed_dim, embed_dim * 2, kernel_size=2, stride=2)
        self.norm = nn.LayerNorm(embed_dim * 2)
        
        # --- Level 2: æŒ‡çº¹ç½‘æ ¼ (4x6) ---
        self.l2_swins = nn.ModuleList([
            SwinTransformerBlock(dim=embed_dim * 2, input_resolution=(4, 6), num_heads=8, window_size=2, shift_size=0),
            SwinTransformerBlock(dim=embed_dim * 2, input_resolution=(4, 6), num_heads=8, window_size=2, shift_size=1)
        ])
        
        self.variable_attention = nn.TransformerEncoderLayer(d_model=embed_dim * 2, nhead=8, 
                                                             dim_feedforward=embed_dim * 8, batch_first=True, norm_first=True)
        self.head = nn.Linear(embed_dim * 2, 96)

    def forward(self, x):
        B, L, N = x.shape
        x = self.revin(x, 'norm')
        x = self.patch_embed(x.permute(0, 2, 1).reshape(B * N, L, 1)).view(B * N, self.H, self.W, -1)
        
        # --- Level 1 æ‰§è¡Œ ---
        x = self.l1_swin1(x)
        # æ³¨å…¥ä¸­é—´æ‰°åŠ¨
        if self.training:
            x = x + torch.randn_like(x) * 0.02
        x = self.l1_swin2(x)
        # DAE é‡å»ºä»»åŠ¡
        x_flat, d_loss = self.dae_block(x.view(B * N, L, -1))
        x = x_flat.view(B * N, self.H, self.W, -1)
        
        # --- ä¸‹é‡‡æ · ---
        x = self.reduction(x.permute(0, 3, 1, 2)).flatten(2).transpose(1, 2)
        x = self.norm(x).view(-1, 4, 6, 256)
        
        # --- Level 2 æ‰§è¡Œ ---
        for blk in self.l2_swins: x = blk(x)
        
        # å…¨å±€å˜é‡å…³è”
        feat = x.mean(dim=(1, 2)).view(B, N, -1)
        feat = self.variable_attention(feat)
        out = self.head(feat).permute(0, 2, 1)
        return self.revin(out, 'denorm'), d_loss

# ==========================================
# 3. è¾…åŠ©ç»„ä»¶ï¼šRevIN ä¸ Dataset
# ==========================================
class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True):
        super(RevIN, self).__init__()
        self.num_features = num_features; self.eps = eps; self.affine = affine
        if self.affine:
            self.affine_weight = nn.Parameter(torch.ones(self.num_features))
            self.affine_bias = nn.Parameter(torch.zeros(self.num_features))
    def forward(self, x, mode:str):
        if mode == 'norm':
            dim2reduce = tuple(range(len(x.shape)-1))
            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
            self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()
            x = (x - self.mean) / self.stdev
            if self.affine: x = x * self.affine_weight + self.affine_bias
        elif mode == 'denorm':
            if self.affine: x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)
            x = x * self.stdev + self.mean
        return x

class ElectricityDataset(Dataset):
    def __init__(self, csv_path, flag='train'):
        df_raw = pd.read_csv(csv_path).iloc[:, 1:]
        n = len(df_raw); tr = int(n*0.7); te = int(n*0.2); val = n - tr - te
        b1s = [0, tr - 96, n - te - 96]; b2s = [tr, tr + val, n]
        f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
        self.scaler = StandardScaler(); self.scaler.fit(df_raw.iloc[b1s[0]:b2s[0]].values)
        self.data = self.scaler.transform(df_raw.values)[b1s[f_idx]:b2s[f_idx]]
    def __getitem__(self, i):
        return torch.tensor(self.data[i:i+96], dtype=torch.float32), torch.tensor(self.data[i+96:i+192], dtype=torch.float32)
    def __len__(self): return len(self.data) - 191

# ==========================================
# 4. åˆ†å¸ƒå¼è®­ç»ƒä¸»ç¨‹åº
# ==========================================
def main():
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    dist.init_process_group("nccl"); torch.cuda.set_device(local_rank)
    if local_rank == 0:
        p = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
        with open(".ds_p.tmp", "w") as f: f.write(p)
    dist.barrier()
    with open(".ds_p.tmp", "r") as f: csv_p = glob.glob(os.path.join(f.read().strip(), "**", "electricity.csv"), recursive=True)[0]

    train_ds = ElectricityDataset(csv_p, 'train'); val_ds = ElectricityDataset(csv_p, 'val')
    train_loader = DataLoader(train_ds, batch_size=8//dist.get_world_size(), sampler=DistributedSampler(train_ds))
    val_loader = DataLoader(val_ds, batch_size=8//dist.get_world_size(), sampler=DistributedSampler(val_ds, shuffle=False))

    model = Swin_iFold_V6(num_vars=321, embed_dim=128).to(local_rank)
    model = DDP(model, device_ids=[local_rank])
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)
    criterion = nn.HuberLoss(delta=0.5)

    if local_rank == 0: print(f"ğŸš€ V6 [DAE + Hierarchical] å¯åŠ¨ | 4090 å¹¶è¡ŒåŠ é€Ÿ")

    for epoch in range(30):
        train_loader.sampler.set_epoch(epoch); model.train(); epoch_loss = 0
        for bx, by in train_loader:
            bx, by = bx.to(local_rank), by.to(local_rank)
            optimizer.zero_grad(set_to_none=True)
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                # è·å–ä¸»é¢„æµ‹å’Œé‡å»º Loss
                pred, d_loss = model(bx)
                main_loss = criterion(pred, by)
                # 0.1 æ˜¯ DAE é‡å»ºä»»åŠ¡çš„æƒé‡
                total_loss = main_loss + 0.1 * d_loss.mean()
            total_loss.backward(); optimizer.step(); epoch_loss += total_loss.item()
        
        scheduler.step(); dist.barrier()
        if local_rank == 0:
            model.eval(); v_mse = 0
            with torch.no_grad():
                for vbx, vby in val_loader:
                    p, _ = model(vbx.to(local_rank))
                    v_mse += F.mse_loss(p, vby.to(local_rank)).item()
            print(f"E{epoch+1:02d} | Train: {epoch_loss/len(train_loader):.4f} | Val MSE: {v_mse/len(val_loader):.6f}")
            torch.save(model.module.state_dict(), "swin_v6_best.pth")

if __name__ == "__main__": main()





(base) root@ubuntu22:~# OMP_NUM_THREADS=1 torchrun --nproc_per_node=4 --master_port=29588 test2.py
ğŸ“ æ•°æ®è·¯å¾„åŠ è½½æˆåŠŸ: /root/.cache/kagglehub/datasets/tylerfarnan/itransformer-datasets/versions/1/iTransformer_datasets/electricity/electricity.csv
E01 | Train: 0.0848 | Val MSE: 0.150322
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E02 | Train: 0.0634 | Val MSE: 0.147515
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E03 | Train: 0.0587 | Val MSE: 0.130501
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E04 | Train: 0.0564 | Val MSE: 0.124316
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E05 | Train: 0.0563 | Val MSE: 0.123094
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E06 | Train: 0.0557 | Val MSE: 0.123713
E07 | Train: 0.0539 | Val MSE: 0.120723
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E08 | Train: 0.0527 | Val MSE: 0.121340
E09 | Train: 0.0519 | Val MSE: 0.119288
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E10 | Train: 0.0511 | Val MSE: 0.118450
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E11 | Train: 0.0500 | Val MSE: 0.118312
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E12 | Train: 0.0497 | Val MSE: 0.117641
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E13 | Train: 0.0489 | Val MSE: 0.117498
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E14 | Train: 0.0483 | Val MSE: 0.117166
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E15 | Train: 0.0476 | Val MSE: 0.116887
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E16 | Train: 0.0468 | Val MSE: 0.115606
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E17 | Train: 0.0462 | Val MSE: 0.115785
E18 | Train: 0.0459 | Val MSE: 0.115058
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E19 | Train: 0.0454 | Val MSE: 0.114245
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E20 | Train: 0.0452 | Val MSE: 0.114728
E21 | Train: 0.0447 | Val MSE: 0.114140
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E22 | Train: 0.0444 | Val MSE: 0.115260
E23 | Train: 0.0442 | Val MSE: 0.114028
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E24 | Train: 0.0438 | Val MSE: 0.113806
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E25 | Train: 0.0437 | Val MSE: 0.113570
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E26 | Train: 0.0438 | Val MSE: 0.113402
ğŸŒŸ æ–°çºªå½•ï¼å·²ä¿å­˜è‡³ v6_best_model.pth
E27 | Train: 0.0436 | Val MSE: 0.113796
E28 | Train: 0.0433 | Val MSE: 0.113764
E29 | Train: 0.0433 | Val MSE: 0.113542
E30 | Train: 0.0433 | Val MSE: 0.113569


##æµ‹è¯•é›†ç»“æœ
import os, math, random, time, glob, torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from timm.models.swin_transformer import SwinTransformerBlock
from timm.layers import Mlp

# ==========================================
# 1. æ ¸å¿ƒæ¶æ„å®šä¹‰ (å˜é‡åå·²ä¸¥æ ¼å¯¹é½è®­ç»ƒç‰ˆ)
# ==========================================
class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True):
        super(RevIN, self).__init__()
        self.num_features = num_features; self.eps = eps; self.affine = affine
        if self.affine:
            self.affine_weight = nn.Parameter(torch.ones(num_features))
            self.affine_bias = nn.Parameter(torch.zeros(num_features))
    def forward(self, x, mode:str):
        if mode == 'norm':
            dim2reduce = tuple(range(len(x.shape)-1))
            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
            self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()
            x = (x - self.mean) / self.stdev
            if self.affine: x = x * self.affine_weight + self.affine_bias
        elif mode == 'denorm':
            if self.affine: x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)
            x = x * self.stdev + self.mean
        return x

class DenoisingDisruptionBlock(nn.Module):
    def __init__(self, dim, num_heads=8, noise_level=0.1): 
        super().__init__()
        self.norm1 = nn.LayerNorm(dim); self.norm2 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)
        self.mlp = Mlp(in_features=dim, hidden_features=dim*4, out_features=dim)
        self.noise_level = noise_level
        self.denoise_loss_fn = nn.MSELoss()
        self.denoise_proj = nn.Linear(dim, dim)
    def forward(self, x):
        # æ¨ç†é˜¶æ®µä¸åŠ å™ªå£°
        attn_out, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))
        x_p = x + attn_out
        x_p = x_p + self.mlp(self.norm2(x_p))
        return x_p, torch.tensor(0.0, device=x.device)

class Swin_iFold_V6(nn.Module):
    def __init__(self, num_vars=321, embed_dim=128):
        super().__init__()
        self.H, self.W = 8, 12; self.revin = RevIN(num_vars); self.patch_embed = nn.Linear(1, embed_dim)
        
        # --- Level 1 ---
        self.l1_swin1 = SwinTransformerBlock(dim=embed_dim, input_resolution=(8,12), num_heads=8, window_size=4, shift_size=0)
        self.l1_swin2 = SwinTransformerBlock(dim=embed_dim, input_resolution=(8,12), num_heads=8, window_size=4, shift_size=2)
        self.dae_block = DenoisingDisruptionBlock(embed_dim, num_heads=8, noise_level=0.1)
        
        # --- Downsample ---
        self.reduction = nn.Conv2d(embed_dim, embed_dim * 2, kernel_size=2, stride=2); self.norm = nn.LayerNorm(embed_dim * 2)
        
        # --- Level 2 ---
        self.l2_swins = nn.ModuleList([
            SwinTransformerBlock(dim=embed_dim * 2, input_resolution=(4,6), num_heads=8, window_size=2, shift_size=0),
            SwinTransformerBlock(dim=embed_dim * 2, input_resolution=(4,6), num_heads=8, window_size=2, shift_size=1)
        ])
        # æ­¤å¤„å˜é‡åå·²ä¿®å¤å› variable_attention
        self.variable_attention = nn.TransformerEncoderLayer(d_model=embed_dim * 2, nhead=8, dim_feedforward=embed_dim * 8, batch_first=True, norm_first=True)
        self.head = nn.Linear(embed_dim * 2, 96)

    def forward(self, x):
        B, L, N = x.shape
        x = self.revin(x, 'norm')
        x = self.patch_embed(x.permute(0, 2, 1).reshape(B * N, L, 1)).view(B * N, self.H, self.W, -1)
        
        x = self.l1_swin1(x)
        x = self.l1_swin2(x)
        x_flat, d_loss = self.dae_block(x.view(B * N, L, -1))
        x = x_flat.view(B * N, self.H, self.W, -1)
        
        # ä¸‹é‡‡æ ·å¹¶å±•å¹³
        x = self.reduction(x.permute(0, 3, 1, 2)).flatten(2).transpose(1, 2)
        x = self.norm(x).view(-1, 4, 6, 256)
        
        for blk in self.l2_swins: x = blk(x)
        feat = x.mean(dim=(1, 2)).view(B, N, -1)
        feat = self.variable_attention(feat) # è¿™é‡Œä¹Ÿå·²åŒæ­¥
        out = self.head(feat).permute(0, 2, 1)
        return self.revin(out, 'denorm'), d_loss

class ElectricityDataset(Dataset):
    def __init__(self, csv_path, flag='train'):
        df_raw = pd.read_csv(csv_path).iloc[:, 1:]
        n = len(df_raw); tr = int(n*0.7); te = int(n*0.2); val = n - tr - te
        b1s = [0, tr - 96, n - te - 96]; b2s = [tr, tr + val, n]
        f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
        self.scaler = StandardScaler(); self.scaler.fit(df_raw.iloc[b1s[0]:b2s[0]].values)
        self.data = self.scaler.transform(df_raw.values)[b1s[f_idx]:b2s[f_idx]]
    def __getitem__(self, i): return torch.tensor(self.data[i:i+96], dtype=torch.float32), torch.tensor(self.data[i+96:i+192], dtype=torch.float32)
    def __len__(self): return len(self.data) - 191

# ==========================================
# 2. ç›²æµ‹æ‰§è¡Œå‡½æ•°
# ==========================================
def run_final_test(checkpoint_path="v6_best_model.pth"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“¡ å¯åŠ¨æµ‹è¯•ç¨‹åºï¼Œä½¿ç”¨è®¾å¤‡: {device}")

    with open(".ds_p.tmp", "r") as f: base_p = f.read().strip()
    csv_p = glob.glob(os.path.join(base_p, "**", "electricity.csv"), recursive=True)[0]
    
    test_ds = ElectricityDataset(csv_p, flag='test')
    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)
    
    model = Swin_iFold_V6(num_vars=321, embed_dim=128).to(device)
    
    # æ³¨æ„ï¼šå¦‚æœè®­ç»ƒæ—¶æ˜¯ DDP æ¨¡å‹ï¼Œæƒé‡åå¯èƒ½ä¼šå¸¦ 'module.' å‰ç¼€ï¼Œ
    # è¿™é‡Œé€šè¿‡ä¸€ä¸ªå° trick è‡ªåŠ¨å¤„ç†
    state_dict = torch.load(checkpoint_path, map_location=device)
    new_state_dict = {}
    for k, v in state_dict.items():
        name = k[7:] if k.startswith('module.') else k # å»æ‰ module. å‰ç¼€
        new_state_dict[name] = v
        
    model.load_state_dict(new_state_dict)
    model.eval()
    print(f"âœ… åŠ è½½æƒé‡æˆåŠŸ: {checkpoint_path}")

    total_mse, total_mae = 0, 0
    preds_to_plot, true_to_plot = None, None
    
    with torch.no_grad():
        for i, (bx, by) in enumerate(test_loader):
            bx, by = bx.to(device), by.to(device)
            pred, _ = model(bx) 
            total_mse += F.mse_loss(pred, by).item()
            total_mae += F.l1_loss(pred, by).item()
            if i == 50: # æ¢ä¸ª batch çœ‹çœ‹é¢„æµ‹æ•ˆæœ
                preds_to_plot = pred.cpu().numpy()
                true_to_plot = by.cpu().numpy()

    avg_mse = total_mse / len(test_loader)
    avg_mae = total_mae / len(test_loader)

    print("-" * 30)
    print(f"ğŸ† ã€æœ€ç»ˆæµ‹è¯•é›†ç›²æµ‹æˆç»©ã€‘")
    print(f"ğŸ“Š Test MSE: {avg_mse:.6f}")
    print(f"ğŸ“ˆ Test MAE: {avg_mae:.6f}")
    print("-" * 30)

    # ç»˜åˆ¶å¯è§†åŒ–æ›²çº¿
    plt.figure(figsize=(10, 4))
    plt.plot(true_to_plot[0, :, 0], label='Actual', color='blue', alpha=0.5, linewidth=2)
    plt.plot(preds_to_plot[0, :, 0], label='V6 Predict', color='red', linestyle='--', linewidth=2)
    plt.legend(); plt.grid(True, alpha=0.3); plt.title("Swin-iFold V6 Final Test Prediction")
    plt.savefig("test_prediction_final.png")
    print("ğŸ¨ é¢„æµ‹å¯¹æ¯”å›¾å·²ä¿å­˜: test_prediction_final.png")

if __name__ == "__main__":
    run_final_test()

(base) root@ubuntu22:~# python eval_test.py 
ğŸ“¡ å¯åŠ¨æµ‹è¯•ç¨‹åºï¼Œä½¿ç”¨è®¾å¤‡: cuda
âœ… åŠ è½½æƒé‡æˆåŠŸ: v6_best_model.pth
------------------------------
ğŸ† ã€æœ€ç»ˆæµ‹è¯•é›†ç›²æµ‹æˆç»©ã€‘
ğŸ“Š Test MSE: 0.134927
ğŸ“ˆ Test MAE: 0.228084




==================================================================================================================================================================================
base) root@ubuntu22:~# torchrun --nproc_per_node=4 swinifold.py
W0218 02:24:19.105000 6481 site-packages/torch/distributed/run.py:793] 
W0218 02:24:19.105000 6481 site-packages/torch/distributed/run.py:793] *****************************************
W0218 02:24:19.105000 6481 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0218 02:24:19.105000 6481 site-packages/torch/distributed/run.py:793] *****************************************
[rank0]:[W218 02:24:22.973160678 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W218 02:24:22.990939909 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W218 02:24:22.991651293 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W218 02:24:22.991802182 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
============================================================
ğŸš€ V6 [çœŸÂ·åŒé‡é˜²å¾¡ç‰ˆ] å¯åŠ¨ | åŸºç¡€å™ªå£° + æ··åˆç ´å DAE
============================================================
E01 | Time: 61.9s | T-Loss: 0.0809 | DAE-Loss: 0.11458 | Val MSE: 0.150839 ğŸŒŸ New Best
E02 | Time: 61.3s | T-Loss: 0.0619 | DAE-Loss: 0.23455 | Val MSE: 0.145654 ğŸŒŸ New Best
E03 | Time: 61.4s | T-Loss: 0.0580 | DAE-Loss: 0.33443 | Val MSE: 0.132131 ğŸŒŸ New Best
E04 | Time: 61.2s | T-Loss: 0.0561 | DAE-Loss: 0.32876 | Val MSE: 0.127597 ğŸŒŸ New Best
E05 | Time: 61.4s | T-Loss: 0.0552 | DAE-Loss: 0.32138 | Val MSE: 0.126774 ğŸŒŸ New Best
E06 | Time: 61.4s | T-Loss: 0.0537 | DAE-Loss: 0.29068 | Val MSE: 0.124729 ğŸŒŸ New Best
E07 | Time: 61.4s | T-Loss: 0.0528 | DAE-Loss: 0.23114 | Val MSE: 0.121589 ğŸŒŸ New Best
E08 | Time: 61.3s | T-Loss: 0.0514 | DAE-Loss: 0.18572 | Val MSE: 0.123512 
E09 | Time: 61.6s | T-Loss: 0.0504 | DAE-Loss: 0.15257 | Val MSE: 0.119850 ğŸŒŸ New Best
E10 | Time: 61.5s | T-Loss: 0.0495 | DAE-Loss: 0.14164 | Val MSE: 0.118757 ğŸŒŸ New Best
E11 | Time: 62.1s | T-Loss: 0.0482 | DAE-Loss: 0.14050 | Val MSE: 0.118805 
E12 | Time: 61.3s | T-Loss: 0.0479 | DAE-Loss: 0.13748 | Val MSE: 0.117813 ğŸŒŸ New Best
E13 | Time: 61.6s | T-Loss: 0.0469 | DAE-Loss: 0.13145 | Val MSE: 0.117504 ğŸŒŸ New Best
E14 | Time: 61.5s | T-Loss: 0.0464 | DAE-Loss: 0.12284 | Val MSE: 0.116882 ğŸŒŸ New Best
E15 | Time: 61.4s | T-Loss: 0.0458 | DAE-Loss: 0.12044 | Val MSE: 0.116179 ğŸŒŸ New Best
E16 | Time: 61.3s | T-Loss: 0.0450 | DAE-Loss: 0.11797 | Val MSE: 0.115395 ğŸŒŸ New Best
E17 | Time: 61.4s | T-Loss: 0.0443 | DAE-Loss: 0.11217 | Val MSE: 0.115882 
E18 | Time: 61.4s | T-Loss: 0.0441 | DAE-Loss: 0.11035 | Val MSE: 0.114761 ğŸŒŸ New Best
E19 | Time: 61.4s | T-Loss: 0.0435 | DAE-Loss: 0.10623 | Val MSE: 0.114218 ğŸŒŸ New Best
E20 | Time: 61.4s | T-Loss: 0.0432 | DAE-Loss: 0.10083 | Val MSE: 0.113839 ğŸŒŸ New Best
E21 | Time: 61.4s | T-Loss: 0.0427 | DAE-Loss: 0.10087 | Val MSE: 0.114327 
E22 | Time: 61.4s | T-Loss: 0.0423 | DAE-Loss: 0.09685 | Val MSE: 0.113924 
E23 | Time: 61.4s | T-Loss: 0.0421 | DAE-Loss: 0.09340 | Val MSE: 0.113876 
E24 | Time: 61.6s | T-Loss: 0.0417 | DAE-Loss: 0.09380 | Val MSE: 0.113330 ğŸŒŸ New Best
E25 | Time: 61.4s | T-Loss: 0.0415 | DAE-Loss: 0.09416 | Val MSE: 0.113164 ğŸŒŸ New Best
E26 | Time: 61.4s | T-Loss: 0.0415 | DAE-Loss: 0.09274 | Val MSE: 0.113177 
E27 | Time: 61.4s | T-Loss: 0.0415 | DAE-Loss: 0.09142 | Val MSE: 0.112972 ğŸŒŸ New Best
E28 | Time: 61.3s | T-Loss: 0.0412 | DAE-Loss: 0.09106 | Val MSE: 0.112921 ğŸŒŸ New Best
E29 | Time: 61.3s | T-Loss: 0.0412 | DAE-Loss: 0.09101 | Val MSE: 0.112767 ğŸŒŸ New Best
E30 | Time: 61.4s | T-Loss: 0.0412 | DAE-Loss: 0.09098 | Val MSE: 0.112703 ğŸŒŸ New Best

============================================================
ğŸ† è®­ç»ƒç»“æŸï¼åŠ è½½æœ€ä½³æƒé‡ [best_v6_dae_true.pth] è¿›è¡Œç»ˆæµ‹...
============================================================
============================================================
ğŸ† è®­ç»ƒç»“æŸï¼åŠ è½½æœ€ä½³æƒé‡ [best_v6_dae_true.pth] è¿›è¡Œç»ˆæµ‹...
============================================================
============================================================
ğŸ† è®­ç»ƒç»“æŸï¼åŠ è½½æœ€ä½³æƒé‡ [best_v6_dae_true.pth] è¿›è¡Œç»ˆæµ‹...
============================================================
============================================================
ğŸ† è®­ç»ƒç»“æŸï¼åŠ è½½æœ€ä½³æƒé‡ [best_v6_dae_true.pth] è¿›è¡Œç»ˆæµ‹...
============================================================

/root/swinifold.py:265: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.module.load_state_dict(torch.load("best_v6_dae_true.pth", map_location=f"cuda:{local_rank}"))
/root/swinifold.py:265: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.module.load_state_dict(torch.load("best_v6_dae_true.pth", map_location=f"cuda:{local_rank}"))
/root/swinifold.py:265: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.module.load_state_dict(torch.load("best_v6_dae_true.pth", map_location=f"cuda:{local_rank}"))
/root/swinifold.py:265: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.module.load_state_dict(torch.load("best_v6_dae_true.pth", map_location=f"cuda:{local_rank}"))
ğŸ“Š ã€æœ€ç»ˆåŒé‡é˜²å¾¡å¼€å¥–ã€‘
ğŸ”¥ TEST MSE: 0.133400
ğŸ“‰ Best Val MSE: 0.112703








# ======================================================================
# Swin-iFold V6 [çœŸÂ·åŒé‡é˜²å¾¡ç‰ˆ] 
# æ‹“æ‰‘ï¼šSwin1 -> åŸºç¡€å™ªå£°(0.025) -> Swin2 -> æ··åˆå™ªå£°ç ´å(2.5%) -> DAE æé™é‡æ„
# ======================================================================
import os, math, random, time, glob, torch, kagglehub
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
from sklearn.preprocessing import StandardScaler
from timm.models.swin_transformer import SwinTransformerBlock
from timm.layers import Mlp

# ==========================================
# 0. æè‡´æ€§èƒ½é…ç½®
# ==========================================
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# ==========================================
# 1. æ ¸å¿ƒç»„ä»¶ï¼šDAE é‡å»ºå—
# ==========================================
class DenoisingDisruptionBlock(nn.Module):
    def __init__(self, dim, num_heads=8): 
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = Mlp(in_features=dim, hidden_features=dim*4, out_features=dim)
        self.denoise_loss_fn = nn.MSELoss()
        self.denoise_proj = nn.Linear(dim, dim)
        
    def forward(self, x_noisy, x_clean_target=None):
        attn_out, _ = self.attn(self.norm1(x_noisy), self.norm1(x_noisy), self.norm1(x_noisy))
        x_p = x_noisy + attn_out
        x_p = x_p + self.mlp(self.norm2(x_p))
        
        # è®­ç»ƒé˜¶æ®µï¼šDAE åŠªåŠ›å°†ç ´æŸç‰¹å¾è¿˜åŸä¸º clean_target
        if self.training and x_clean_target is not None: 
            # ä½¿ç”¨ detach() é˜»æ–­é‡æ„æŸå¤±å‘ Swin å±‚çš„å›ä¼ 
            d_loss = self.denoise_loss_fn(self.denoise_proj(x_p), x_clean_target.detach())
            return x_p, d_loss
            
        return x_p, torch.tensor(0.0, device=x_noisy.device)

# ==========================================
# 2. çœŸå®æ¶æ„ (åŒé‡å™ªå£°æ³¨å…¥)
# ==========================================
class Swin_iFold_V6(nn.Module):
    def __init__(self, num_vars=321, embed_dim=128, noise_level=0.02): 
        super().__init__()
        self.H, self.W = 8, 12 
        self.revin = RevIN(num_vars)
        self.patch_embed = nn.Linear(1, embed_dim)
        self.noise_level = noise_level
        
        # Level 1 Swin æ¨¡å—é…ç½®
        self.l1_swin1 = SwinTransformerBlock(dim=embed_dim, input_resolution=(8, 12), num_heads=8, window_size=4, shift_size=0, proj_drop=0.025, attn_drop=0.025)
        self.l1_swin2 = SwinTransformerBlock(dim=embed_dim, input_resolution=(8, 12), num_heads=8, window_size=4, shift_size=2, proj_drop=0.025, attn_drop=0.025)
        
        # DAE ä½äº Level 1 ç»“å°¾
        self.dae_block = DenoisingDisruptionBlock(embed_dim, num_heads=8)
        
        self.reduction = nn.Conv2d(embed_dim, embed_dim * 2, kernel_size=2, stride=2)
        self.norm = nn.LayerNorm(embed_dim * 2)
        
        self.l2_swins = nn.ModuleList([
            SwinTransformerBlock(dim=embed_dim * 2, input_resolution=(4, 6), num_heads=8, window_size=2, shift_size=0, proj_drop=0.025, attn_drop=0.025),
            SwinTransformerBlock(dim=embed_dim * 2, input_resolution=(4, 6), num_heads=8, window_size=2, shift_size=1, proj_drop=0.025, attn_drop=0.025)
        ])
        
        self.variable_attention = nn.TransformerEncoderLayer(
            d_model=embed_dim * 2, nhead=8, dim_feedforward=embed_dim * 8, batch_first=True, norm_first=True, dropout=0.05
        )
        self.head = nn.Linear(embed_dim * 2, 96)

    def inject_mixed_noise(self, x):
        """ä¸º DAE å‡†å¤‡çš„ 2:4:4 æ¯ç­æ€§ç»“æ„åŒ–æ··åˆå™ªå£°"""
        x_noisy = x.clone()
        rand_probs = torch.rand_like(x)
        
        # 1. 20% ä»½é¢çš„é«˜æ–¯å™ªå£° (âœ… å½»åº•ä¿®å¤ In-place æ¢¯åº¦æˆªæ–­ Bug)
        gauss_mask = rand_probs < (self.noise_level * 0.2)
        gauss_noise = torch.randn_like(x) * (self.noise_level) 
        x_noisy = torch.where(gauss_mask, x_noisy + gauss_noise, x_noisy)
        
        # 2. 40% ä»½é¢çš„è„‰å†²æå€¼å™ªå£°
        pulse_mask = (rand_probs >= (self.noise_level * 0.2)) & (rand_probs < (self.noise_level * 0.6))
        spikes = torch.where(torch.rand_like(x) > 0.5, x.max(), x.min())
        x_noisy = torch.where(pulse_mask, spikes, x_noisy)
        
        # 3. 40% ä»½é¢çš„æ¼‚ç§»ç¼©æ”¾å™ªå£°
        drift_mask = (rand_probs >= (self.noise_level * 0.6)) & (rand_probs < self.noise_level)
        scales = 1.0 + torch.randn_like(x) * (self.noise_level)
        x_noisy = torch.where(drift_mask, x_noisy * scales, x_noisy)
        
        return x_noisy

    def forward(self, x):
        B, L, N = x.shape
        x = self.revin(x, 'norm')
        x = self.patch_embed(x.permute(0, 2, 1).reshape(B * N, L, 1)).view(B * N, self.H, self.W, -1)
        
        # âœ… ç¬¬ä¸€é‡é˜²å¾¡ï¼šSwin1 æå–åˆæ­¥ç‰¹å¾
        x = self.l1_swin1(x)
        
        # âœ… æ³¨å…¥ç¬¬ä¸€çº§å¾®å‰‚é‡åº•ç›˜å™ªå£° (é˜²è¿‡æ‹Ÿåˆ)
        if self.training:
            x = x + torch.randn_like(x) * self.noise_level*0.75
            
        # âœ… Swin2 è·¨çª—å£å±€éƒ¨ç‰¹å¾äº¤äº’ï¼Œè¾“å‡ºâ€œå¹²å‡€é¶æ ‡â€
        x_clean_target = self.l1_swin2(x)
        
        # âœ… ç¬¬äºŒé‡é˜²å¾¡ï¼šä¸º DAE æ³¨å…¥ 2.5% è¦†ç›–ç‡çš„æ··åˆç»“æ„åŒ–ç ´å
        if self.training:
            x_noisy = self.inject_mixed_noise(x_clean_target)
        else:
            x_noisy = x_clean_target
            
        # âœ… DAE å…¨å±€ç²¾ä¿®ä¸ Loss è®¡ç®—
        x_flat, d_loss = self.dae_block(x_noisy.view(B * N, L, -1), x_clean_target.view(B * N, L, -1))
        x = x_flat.view(B * N, self.H, self.W, -1)
        
        # --- ä¸‹é‡‡æ ·ä¸ Level 2 å˜é‡åšå¼ˆ ---
        x = self.reduction(x.permute(0, 3, 1, 2)).flatten(2).transpose(1, 2)
        x = self.norm(x).view(-1, 4, 6, 256)
        
        for blk in self.l2_swins: x = blk(x)
        
        feat = x.mean(dim=(1, 2)).view(B, N, -1)
        feat = self.variable_attention(feat)
        out = self.head(feat).permute(0, 2, 1)
        
        return self.revin(out, 'denorm'), d_loss

# ==========================================
# 3. è¾…åŠ©ç»„ä»¶
# ==========================================
class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True):
        super(RevIN, self).__init__()
        self.num_features = num_features; self.eps = eps; self.affine = affine
        if self.affine:
            self.affine_weight = nn.Parameter(torch.ones(self.num_features))
            self.affine_bias = nn.Parameter(torch.zeros(self.num_features))
            
    def forward(self, x, mode:str):
        if mode == 'norm':
            dim2reduce = tuple(range(len(x.shape)-1))
            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
            self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()
            x = (x - self.mean) / self.stdev
            if self.affine: x = x * self.affine_weight + self.affine_bias
        elif mode == 'denorm':
            # âœ… å·²ä¿®å¤çš„é™¤é›¶ Bug
            if self.affine: x = (x - self.affine_bias) / (self.affine_weight + self.eps)
            x = x * self.stdev + self.mean
        return x

class ElectricityDataset(Dataset):
    def __init__(self, csv_path, flag='train'):
        df_raw = pd.read_csv(csv_path).iloc[:, 1:]
        n = len(df_raw); tr = int(n*0.7); te = int(n*0.2); val = n - tr - te
        b1s = [0, tr - 96, n - te - 96]; b2s = [tr, tr + val, n]
        f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
        
        self.scaler = StandardScaler(); self.scaler.fit(df_raw.iloc[0:tr].values) 
        self.data = self.scaler.transform(df_raw.values)[b1s[f_idx]:b2s[f_idx]]
        
    def __getitem__(self, i):
        return torch.tensor(self.data[i:i+96], dtype=torch.float32), torch.tensor(self.data[i+96:i+192], dtype=torch.float32)
        
    def __len__(self): return len(self.data) - 191

# ==========================================
# 4. åˆ†å¸ƒå¼ä¸»ç¨‹åº + è‡ªåŠ¨å¼€å¥–
# ==========================================
def main():
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    dist.init_process_group("nccl"); torch.cuda.set_device(local_rank)
    
    dataset_path_file = ".kaggle_path.tmp"
    if local_rank == 0:
        if not os.path.exists(dataset_path_file):
            path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
            with open(dataset_path_file, "w") as f: f.write(path)
    dist.barrier()
    with open(dataset_path_file, "r") as f: 
        csv_p = glob.glob(os.path.join(f.read().strip(), "**", "electricity.csv"), recursive=True)[0]

    GLOBAL_BS = 8
    train_loader = DataLoader(ElectricityDataset(csv_p, 'train'), batch_size=GLOBAL_BS//dist.get_world_size(), sampler=DistributedSampler(ElectricityDataset(csv_p, 'train')), num_workers=2, pin_memory=True)
    val_loader = DataLoader(ElectricityDataset(csv_p, 'val'), batch_size=GLOBAL_BS//dist.get_world_size(), sampler=DistributedSampler(ElectricityDataset(csv_p, 'val'), shuffle=False), num_workers=2, pin_memory=True)
    test_loader = DataLoader(ElectricityDataset(csv_p, 'test'), batch_size=GLOBAL_BS//dist.get_world_size(), sampler=DistributedSampler(ElectricityDataset(csv_p, 'test'), shuffle=False), num_workers=2, pin_memory=True)

    # ç”œç‚¹åŒºå™ªå£°å¼ºåº¦ 0.025
    model = Swin_iFold_V6(num_vars=321, embed_dim=128, noise_level=0.025).to(local_rank)
    model = DDP(model, device_ids=[local_rank], broadcast_buffers=False)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)
    criterion = nn.HuberLoss(delta=0.5)

    if local_rank == 0: print(f"{'='*60}\nğŸš€ V6 [çœŸÂ·åŒé‡é˜²å¾¡ç‰ˆ] å¯åŠ¨ | åŸºç¡€å™ªå£° + æ··åˆç ´å DAE\n{'='*60}")

    best_val_mse = float('inf')
    for epoch in range(30):
        train_loader.sampler.set_epoch(epoch); model.train()
        epoch_main_loss = 0; epoch_d_loss = 0; t0 = time.time()
        
        for bx, by in train_loader:
            bx, by = bx.to(local_rank), by.to(local_rank)
            optimizer.zero_grad(set_to_none=True)
            
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                pred, d_loss = model(bx)
                main_loss = criterion(pred, by)
                total_loss = main_loss + 0.1 * d_loss.mean()
            
            # âœ… å‰¥ç¦» autocast ä»¥ä¿è¯æ¢¯åº¦ç²¾åº¦
            total_loss.backward()
            optimizer.step()
            
            epoch_main_loss += main_loss.item()
            epoch_d_loss += d_loss.mean().item()
            
        model.eval(); local_mse_sum, local_samples = torch.tensor(0.0, device=local_rank), torch.tensor(0.0, device=local_rank)
        with torch.no_grad():
            for vbx, vby in val_loader:
                vbx, vby = vbx.to(local_rank), vby.to(local_rank)
                with autocast(device_type='cuda', dtype=torch.bfloat16):
                    p, _ = model(vbx)
                local_mse_sum += F.mse_loss(p, vby) * vbx.size(0)
                local_samples += vbx.size(0)
                
        dist.all_reduce(local_mse_sum); dist.all_reduce(local_samples)
        avg_val_mse = (local_mse_sum / local_samples).item()
        
        if local_rank == 0:
            avg_m_loss = epoch_main_loss / len(train_loader)
            avg_d_loss = epoch_d_loss / len(train_loader)
            status = "ğŸŒŸ New Best" if avg_val_mse < best_val_mse else ""
            print(f"E{epoch+1:02d} | Time: {time.time()-t0:.1f}s | T-Loss: {avg_m_loss:.4f} | DAE-Loss: {avg_d_loss:.5f} | Val MSE: {avg_val_mse:.6f} {status}")
            
            if avg_val_mse < best_val_mse:
                best_val_mse = avg_val_mse
                torch.save(model.module.state_dict(), "best_v6_dae_true.pth")
                
        scheduler.step()
        # âœ… åŒæ­¥å¢™ï¼šé˜²æ­¢ Rank 0 è¿˜åœ¨ä¿å­˜ç¡¬ç›˜æ—¶ï¼Œå…¶ä»–å¡æŠ¢è·‘å¯¼è‡´é€šä¿¡æŒ‚èµ·
        dist.barrier()

    # ==========================================
    # ğŸ ç»ˆæå¼€å¥–
    # ==========================================
    dist.barrier()
    
    print(f"\n{'='*60}\nğŸ† è®­ç»ƒç»“æŸï¼åŠ è½½æœ€ä½³æƒé‡ [best_v6_dae_true.pth] è¿›è¡Œç»ˆæµ‹...\n{'='*60}")
    model.module.load_state_dict(torch.load("best_v6_dae_true.pth", map_location=f"cuda:{local_rank}"))
    
    model.eval()
    t_mse, t_samples = torch.tensor(0.0, device=local_rank), torch.tensor(0.0, device=local_rank)
    with torch.no_grad():
        for tbx, tby in test_loader:
            tbx, tby = tbx.to(local_rank), tby.to(local_rank)
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                p, _ = model(tbx)
            t_mse += F.mse_loss(p, tby) * tbx.size(0)
            t_samples += tbx.size(0)
            
    dist.all_reduce(t_mse); dist.all_reduce(t_samples)

    if local_rank == 0:
        final_test_mse = (t_mse/t_samples).item()
        print(f"ğŸ“Š ã€æœ€ç»ˆåŒé‡é˜²å¾¡å¼€å¥–ã€‘\nğŸ”¥ TEST MSE: {final_test_mse:.6f}\nğŸ“‰ Best Val MSE: {best_val_mse:.6f}\n{'='*60}")

if __name__ == "__main__": main()








-------------------------------------------ä¸‹è¾¹çš„ä»£ç æ˜¯ç¥çº§ä»£ç ï¼Œè·‘è¿›0.115ï¼Œlogåœ¨ä»£ç ä¸‹æ–¹
import os, math, random, time, glob, torch, kagglehub
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
from sklearn.preprocessing import StandardScaler
from timm.models.swin_transformer import SwinTransformerBlock

# ==========================================
# 0. ç¯å¢ƒåŠ é€Ÿé…ç½®
# ==========================================
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# ==========================================
# 1. RevIN (å®Œå…¨ä¿ç•™ä½ çš„ä»¿å°„å˜æ¢å®ç°)
# ==========================================
class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True):
        super(RevIN, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.affine = affine
        if self.affine:
            self.affine_weight = nn.Parameter(torch.ones(self.num_features))
            self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

    def forward(self, x, mode:str):
        if mode == 'norm':
            dim2reduce = tuple(range(len(x.shape)-1))
            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
            self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()
            x = (x - self.mean) / self.stdev
            if self.affine: x = x * self.affine_weight + self.affine_bias
        elif mode == 'denorm':
            if self.affine:
                x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)
            x = x * self.stdev + self.mean
        return x

# ==========================================
# 2. å±‚æ¬¡åŒ–ç»„ä»¶ (Digit Generator)
# ==========================================
class BoxDownsample(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.reduction = nn.Conv2d(in_dim, out_dim, kernel_size=2, stride=2)
        self.norm = nn.LayerNorm(out_dim)

    def forward(self, x, H, W):
        B_N = x.shape[0]
        x = x.permute(0, 3, 1, 2) # [B*N, C, H, W]
        x = self.reduction(x)     # [B*N, out_C, 4, 6]
        H_new, W_new = x.shape[2], x.shape[3]
        x = x.flatten(2).transpose(1, 2) 
        return self.norm(x).view(B_N, H_new, W_new, -1), H_new, W_new

# ==========================================
# 3. å±‚æ¬¡åŒ– Swin-iFold æ¶æ„
# ==========================================
class Swin_iFold_Hierarchical(nn.Module):
    def __init__(self, num_vars=321, embed_dim=128, depth=2):
        super().__init__()
        self.H, self.W = 8, 12 
        self.revin = RevIN(num_vars)
        self.patch_embed = nn.Linear(1, embed_dim)
        
        # Stage 1: 8x12 åˆ†è¾¨ç‡
        self.swin_stage1 = nn.ModuleList([
            SwinTransformerBlock(dim=embed_dim, input_resolution=(8, 12), num_heads=8, window_size=4,
                                 shift_size=0 if (i % 2 == 0) else 2) for i in range(depth)
        ])
        
        self.downsample = BoxDownsample(embed_dim, embed_dim * 2)
        
        # Stage 2: 4x6 æŒ‡çº¹åˆ†è¾¨ç‡
        self.swin_stage2 = nn.ModuleList([
            SwinTransformerBlock(dim=embed_dim * 2, input_resolution=(4, 6), num_heads=8, window_size=2,
                                 shift_size=0 if (i % 2 == 0) else 1) for i in range(depth)
        ])
        
        self.variable_attention = nn.TransformerEncoderLayer(d_model=embed_dim * 2, nhead=8, 
                                                             dim_feedforward=embed_dim * 8, batch_first=True, norm_first=True)
        self.head = nn.Linear(embed_dim * 2, 96)

    def forward(self, x):
        B, L, N = x.shape
        x = self.revin(x, 'norm')
        x = x.permute(0, 2, 1).reshape(B * N, L, 1)
        x = self.patch_embed(x).view(B * N, self.H, self.W, -1)
        
        for blk in self.swin_stage1: x = blk(x)
        x, H2, W2 = self.downsample(x, self.H, self.W)
        for blk in self.swin_stage2: x = blk(x)
        
        feat = x.mean(dim=(1, 2)).view(B, N, -1)
        feat = self.variable_attention(feat)
        out = self.head(feat).permute(0, 2, 1)
        return self.revin(out, 'denorm')

# ==========================================
# 4. æ•°æ®é›†é€»è¾‘ (å®Œå…¨ä¿ç•™ä½ çš„ Border åˆ’åˆ†)
# ==========================================
class ElectricityDataset(Dataset):
    def __init__(self, csv_path, flag='train'):
        df_raw = pd.read_csv(csv_path).iloc[:, 1:]
        n = len(df_raw); tr = int(n*0.7); te = int(n*0.2); val = n - tr - te
        border1s = [0, tr - 96, n - te - 96]
        border2s = [tr, tr + val, n]
        f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
        b1, b2 = border1s[f_idx], border2s[f_idx]
        self.scaler = StandardScaler()
        self.scaler.fit(df_raw.iloc[border1s[0]:border2s[0]].values)
        self.data = self.scaler.transform(df_raw.values)[b1:b2]

    def __getitem__(self, i):
        return torch.tensor(self.data[i:i+96], dtype=torch.float32), torch.tensor(self.data[i+96:i+192], dtype=torch.float32)

    def __len__(self): return len(self.data) - 191

# ==========================================
# 5. ä¸»è®­ç»ƒå¾ªç¯ (é™é»˜è¾“å‡ºç‰ˆ)
# ==========================================
def main():
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    dist.init_process_group("nccl"); torch.cuda.set_device(local_rank)
    
    if local_rank == 0:
        p = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
        with open(".ds_p.tmp", "w") as f: f.write(p)
    dist.barrier()
    with open(".ds_p.tmp", "r") as f: 
        csv_p = glob.glob(os.path.join(f.read().strip(), "**", "electricity.csv"), recursive=True)[0]

    train_ds = ElectricityDataset(csv_p, 'train'); val_ds = ElectricityDataset(csv_p, 'val')
    train_loader = DataLoader(train_ds, batch_size=8//dist.get_world_size(), sampler=DistributedSampler(train_ds))
    val_loader = DataLoader(val_ds, batch_size=8//dist.get_world_size(), sampler=DistributedSampler(val_ds, shuffle=False))

    model = Swin_iFold_Hierarchical(num_vars=321, embed_dim=128).to(local_rank)
    model = DDP(model, device_ids=[local_rank])
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)
    criterion = nn.HuberLoss() # ä¿æŒä½ çš„ HuberLoss é…ç½®

    if local_rank == 0:
        print(f"ğŸš€ Swin-iFold Hier-V5 å¯åŠ¨ | 4å¡å¹¶è¡Œæ¨¡å¼ | ç›®æ ‡è®°å½• 0.1176")

    for epoch in range(30):
        train_loader.sampler.set_epoch(epoch)
        model.train()
        epoch_loss = 0
        
        for bx, by in train_loader:
            bx, by = bx.to(local_rank), by.to(local_rank)
            optimizer.zero_grad(set_to_none=True)
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                loss = criterion(model(bx), by)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        
        scheduler.step()
        dist.barrier()
        
        if local_rank == 0:
            model.eval()
            v_mse = 0
            with torch.no_grad():
                for vbx, vby in val_loader:
                    v_mse += F.mse_loss(model(vbx.to(local_rank)), vby.to(local_rank)).item()
            avg_loss = epoch_loss / len(train_loader)
            avg_mse = v_mse / len(val_loader)
            print(f"E{epoch+1:02d} | Train Loss: {avg_loss:.4f} | Val MSE: {avg_mse:.6f} | LR: {optimizer.param_groups[0]['lr']:.6f}")
            torch.save(model.module.state_dict(), "swin_hier_best.pth")

if __name__ == "__main__": main()


(base) root@ubuntu22:~# OMP_NUM_THREADS=1 torchrun --nproc_per_node=4 --master_port=29501 test.py
ğŸš€ Swin-iFold Hier-V5 å¯åŠ¨ | 4å¡å¹¶è¡Œæ¨¡å¼ | ç›®æ ‡è®°å½• 0.1176
E01 | Train Loss: 0.0980 | Val MSE: 0.145173 | LR: 0.000997
E02 | Train Loss: 0.0724 | Val MSE: 0.141391 | LR: 0.000989
E03 | Train Loss: 0.0674 | Val MSE: 0.128422 | LR: 0.000976
cE04 | Train Loss: 0.0651 | Val MSE: 0.124317 | LR: 0.000957
E05 | Train Loss: 0.0639 | Val MSE: 0.122289 | LR: 0.000933
E06 | Train Loss: 0.0620 | Val MSE: 0.123123 | LR: 0.000905
E07 | Train Loss: 0.0607 | Val MSE: 0.120685 | LR: 0.000872
E08 | Train Loss: 0.0586 | Val MSE: 0.119430 | LR: 0.000835
E09 | Train Loss: 0.0570 | Val MSE: 0.118504 | LR: 0.000794
E10 | Train Loss: 0.0556 | Val MSE: 0.119562 | LR: 0.000750
E11 | Train Loss: 0.0541 | Val MSE: 0.119140 | LR: 0.000703
E12 | Train Loss: 0.0532 | Val MSE: 0.117039 | LR: 0.000655
E13 | Train Loss: 0.0521 | Val MSE: 0.116496 | LR: 0.000604
E14 | Train Loss: 0.0512 | Val MSE: 0.115512 | LR: 0.000552 #Best Val MSE
E15 | Train Loss: 0.0504 | Val MSE: 0.117546 | LR: 0.000500
E16 | Train Loss: 0.0494 | Val MSE: 0.116832 | LR: 0.000448
E17 | Train Loss: 0.0485 | Val MSE: 0.117217 | LR: 0.000396
E18 | Train Loss: 0.0480 | Val MSE: 0.115856 | LR: 0.000345
E19 | Train Loss: 0.0473 | Val MSE: 0.115713 | LR: 0.000297 
E20 | Train Loss: 0.0469 | Val MSE: 0.118639 | LR: 0.000250
E21 | Train Loss: 0.0462 | Val MSE: 0.116819 | LR: 0.000206
E22 | Train Loss: 0.0459 | Val MSE: 0.117353 | LR: 0.000165
E23 | Train Loss: 0.0455 | Val MSE: 0.118167 | LR: 0.000128
E24 | Train Loss: 0.0450 | Val MSE: 0.117471 | LR: 0.000095
E25 | Train Loss: 0.0448 | Val MSE: 0.117206 | LR: 0.000067
E26 | Train Loss: 0.0447 | Val MSE: 0.117043 | LR: 0.000043
E27 | Train Loss: 0.0446 | Val MSE: 0.117005 | LR: 0.000024
E28 | Train Loss: 0.0443 | Val MSE: 0.117305 | LR: 0.000011
E29 | Train Loss: 0.0442 | Val MSE: 0.117253 | LR: 0.000003
E30 | Train Loss: 0.0443 | Val MSE: 0.117481 | LR: 0.000000


























------------------------------



