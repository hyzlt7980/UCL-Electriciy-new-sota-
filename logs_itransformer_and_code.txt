# ä¸¤ä»½logsçš„é¡ºåºå¯¹åº”ä¸‹è¾¹ä¸¤ä»½ä»£ç çš„é¡ºåº
(base) root@ubuntu22:~# torchrun --nproc_per_node=4 test2.py
[2026-02-15 00:44:02,645] torch.distributed.run: [WARNING] 
[2026-02-15 00:44:02,645] torch.distributed.run: [WARNING] *****************************************
[2026-02-15 00:44:02,645] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2026-02-15 00:44:02,645] torch.distributed.run: [WARNING] *****************************************
ðŸ“¥ Initializing...
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
ðŸš€ iTransformer (HuberLoss) Started | Epochs=30
âœ… Epoch 1/30 | Train HuberLoss: 0.1041 | Val MSE: 0.1638
âœ… Epoch 2/30 | Train HuberLoss: 0.0788 | Val MSE: 0.1470
âœ… Epoch 3/30 | Train HuberLoss: 0.0723 | Val MSE: 0.1381
âœ… Epoch 4/30 | Train HuberLoss: 0.0692 | Val MSE: 0.1317
âœ… Epoch 5/30 | Train HuberLoss: 0.0675 | Val MSE: 0.1295
âœ… Epoch 6/30 | Train HuberLoss: 0.0660 | Val MSE: 0.1283
âœ… Epoch 7/30 | Train HuberLoss: 0.0653 | Val MSE: 0.1286
âœ… Epoch 8/30 | Train HuberLoss: 0.0642 | Val MSE: 0.1261
âœ… Epoch 9/30 | Train HuberLoss: 0.0631 | Val MSE: 0.1250
âœ… Epoch 10/30 | Train HuberLoss: 0.0630 | Val MSE: 0.1266
âœ… Epoch 11/30 | Train HuberLoss: 0.0618 | Val MSE: 0.1268
âœ… Epoch 12/30 | Train HuberLoss: 0.0621 | Val MSE: 0.1231
âœ… Epoch 13/30 | Train HuberLoss: 0.0610 | Val MSE: 0.1247
âœ… Epoch 14/30 | Train HuberLoss: 0.0607 | Val MSE: 0.1248
âœ… Epoch 15/30 | Train HuberLoss: 0.0604 | Val MSE: 0.1235
âœ… Epoch 16/30 | Train HuberLoss: 0.0595 | Val MSE: 0.1235
âœ… Epoch 17/30 | Train HuberLoss: 0.0586 | Val MSE: 0.1229
âœ… Epoch 18/30 | Train HuberLoss: 0.0581 | Val MSE: 0.1237
âœ… Epoch 19/30 | Train HuberLoss: 0.0575 | Val MSE: 0.1229
âœ… Epoch 20/30 | Train HuberLoss: 0.0574 | Val MSE: 0.1233
âœ… Epoch 21/30 | Train HuberLoss: 0.0567 | Val MSE: 0.1224
âœ… Epoch 22/30 | Train HuberLoss: 0.0565 | Val MSE: 0.1237
âœ… Epoch 23/30 | Train HuberLoss: 0.0563 | Val MSE: 0.1218   #best
âœ… Epoch 24/30 | Train HuberLoss: 0.0558 | Val MSE: 0.1224
âœ… Epoch 25/30 | Train HuberLoss: 0.0556 | Val MSE: 0.1222
âœ… Epoch 26/30 | Train HuberLoss: 0.0558 | Val MSE: 0.1225
âœ… Epoch 27/30 | Train HuberLoss: 0.0557 | Val MSE: 0.1223
âœ… Epoch 28/30 | Train HuberLoss: 0.0553 | Val MSE: 0.1226
âœ… Epoch 29/30 | Train HuberLoss: 0.0554 | Val MSE: 0.1222
âœ… Epoch 30/30 | Train HuberLoss: 0.0554 | Val MSE: 0.1223


(base) root@ubuntu22:~# torchrun --nproc_per_node=4 test2.py
[2026-02-15 00:47:24,866] torch.distributed.run: [WARNING] 
[2026-02-15 00:47:24,866] torch.distributed.run: [WARNING] *****************************************
[2026-02-15 00:47:24,866] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2026-02-15 00:47:24,866] torch.distributed.run: [WARNING] *****************************************
ðŸ“¥ Initializing...
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
ðŸš€ iTransformer (Pure MSE) Started | Epochs=30
âœ… Epoch 1/30 | Train MSE: 0.2385 | Val MSE: 0.1645
âœ… Epoch 2/30 | Train MSE: 0.1751 | Val MSE: 0.1481
âœ… Epoch 3/30 | Train MSE: 0.1602 | Val MSE: 0.1410
âœ… Epoch 4/30 | Train MSE: 0.1535 | Val MSE: 0.1346
âœ… Epoch 5/30 | Train MSE: 0.1497 | Val MSE: 0.1320
âœ… Epoch 6/30 | Train MSE: 0.1462 | Val MSE: 0.1317
âœ… Epoch 7/30 | Train MSE: 0.1445 | Val MSE: 0.1309
âœ… Epoch 8/30 | Train MSE: 0.1417 | Val MSE: 0.1296
âœ… Epoch 9/30 | Train MSE: 0.1389 | Val MSE: 0.1280
âœ… Epoch 10/30 | Train MSE: 0.1383 | Val MSE: 0.1303
âœ… Epoch 11/30 | Train MSE: 0.1352 | Val MSE: 0.1294
âœ… Epoch 12/30 | Train MSE: 0.1355 | Val MSE: 0.1250
âœ… Epoch 13/30 | Train MSE: 0.1323 | Val MSE: 0.1261
âœ… Epoch 14/30 | Train MSE: 0.1319 | Val MSE: 0.1266
âœ… Epoch 15/30 | Train MSE: 0.1300 | Val MSE: 0.1257
âœ… Epoch 16/30 | Train MSE: 0.1283 | Val MSE: 0.1264
âœ… Epoch 17/30 | Train MSE: 0.1265 | Val MSE: 0.1254
âœ… Epoch 18/30 | Train MSE: 0.1260 | Val MSE: 0.1261
âœ… Epoch 19/30 | Train MSE: 0.1252 | Val MSE: 0.1253
âœ… Epoch 20/30 | Train MSE: 0.1246 | Val MSE: 0.1253
âœ… Epoch 21/30 | Train MSE: 0.1229 | Val MSE: 0.1244 
âœ… Epoch 22/30 | Train MSE: 0.1228 | Val MSE: 0.1250
âœ… Epoch 23/30 | Train MSE: 0.1222 | Val MSE: 0.1237
âœ… Epoch 24/30 | Train MSE: 0.1212 | Val MSE: 0.1255
âœ… Epoch 25/30 | Train MSE: 0.1207 | Val MSE: 0.1240
âœ… Epoch 26/30 | Train MSE: 0.1210 | Val MSE: 0.1247
âœ… Epoch 27/30 | Train MSE: 0.1210 | Val MSE: 0.1243
âœ… Epoch 28/30 | Train MSE: 0.1201 | Val MSE: 0.1243
âœ… Epoch 29/30 | Train MSE: 0.1203 | Val MSE: 0.1239  #best
âœ… Epoch 30/30 | Train MSE: 0.1202 | Val MSE: 0.1243




# import os, math, random, time, glob, torch, kagglehub
# import numpy as np
# import pandas as pd
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.distributed as dist
# from torch.nn.parallel import DistributedDataParallel as DDP
# from torch.utils.data.distributed import DistributedSampler
# from torch.utils.data import Dataset, DataLoader
# from torch.amp import autocast
# from sklearn.preprocessing import StandardScaler

# # ==========================================
# # 0. çŽ¯å¢ƒé…ç½®ä¸ŽåŠ é€Ÿ
# # ==========================================
# torch.backends.cuda.matmul.allow_tf32 = True
# torch.backends.cudnn.allow_tf32 = True
# torch.backends.cudnn.benchmark = True

# # ==========================================
# # 1. æ ¸å¿ƒç»„ä»¶ï¼šRevIN 
# # ==========================================
# class RevIN(nn.Module):
#     def __init__(self, num_features: int, eps=1e-5, affine=True):
#         super(RevIN, self).__init__()
#         self.num_features = num_features
#         self.eps = eps
#         self.affine = affine
#         if self.affine: self._init_params()

#     def _init_params(self):
#         self.affine_weight = nn.Parameter(torch.ones(self.num_features))
#         self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

#     def forward(self, x, mode:str):
#         if mode == 'norm':
#             self._get_statistics(x)
#             x = self._normalize(x)
#         elif mode == 'denorm':
#             x = self._denormalize(x)
#         return x

#     def _get_statistics(self, x):
#         dim2reduce = tuple(range(len(x.shape)-1))
#         self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
#         self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()

#     def _normalize(self, x):
#         x = (x - self.mean) / self.stdev
#         if self.affine: x = x * self.affine_weight + self.affine_bias
#         return x

#     def _denormalize(self, x):
#         if self.affine:
#             x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)
#         x = x * self.stdev + self.mean
#         return x

# # ==========================================
# # 2. åŽŸç”Ÿ iTransformer æž¶æž„
# # ==========================================
# class iTransformer(nn.Module):
#     def __init__(self, seq_len=96, pred_len=96, num_vars=321, d_model=128, n_heads=8, e_layers=3, d_ff=512, dropout=0.1):
#         super().__init__()
#         self.seq_len = seq_len
#         self.pred_len = pred_len
#         self.num_vars = num_vars
        
#         self.revin = RevIN(num_vars)
#         self.enc_embedding = nn.Linear(seq_len, d_model)
        
#         encoder_layer = nn.TransformerEncoderLayer(
#             d_model=d_model, nhead=n_heads, dim_feedforward=d_ff, 
#             dropout=dropout, batch_first=True, norm_first=True
#         )
#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)
#         self.projector = nn.Linear(d_model, pred_len)

#     def forward(self, x):
#         x = self.revin(x, 'norm')
#         # [B, L, N] -> [B, N, L]  Inverted ç¿»è½¬ï¼
#         x = x.permute(0, 2, 1)
        
#         enc_out = self.enc_embedding(x)
#         enc_out = self.encoder(enc_out)
#         dec_out = self.projector(enc_out)
        
#         dec_out = dec_out.permute(0, 2, 1)
#         dec_out = self.revin(dec_out, 'denorm')
#         return dec_out

# # ==========================================
# # 3. æ•°æ®é›†å®šä¹‰ 
# # ==========================================
# class ElectricityDataset(Dataset):
#     def __init__(self, csv_path, flag='train'):
#         self.seq_len = 96; self.pred_len = 96
#         df_raw = pd.read_csv(csv_path)
#         df_data = df_raw.iloc[:, 1:] 
#         n = len(df_data)
#         num_train = int(n * 0.7); num_test = int(n * 0.2); num_val = n - num_train - num_test
        
#         border1s = [0, num_train - self.seq_len, n - num_test - self.seq_len]
#         border2s = [num_train, num_train + num_val, n]
#         f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
#         b1, b2 = border1s[f_idx], border2s[f_idx]
        
#         self.scaler = StandardScaler()
#         train_data = df_data.iloc[border1s[0]:border2s[0]]
#         self.scaler.fit(train_data.values)
#         data = self.scaler.transform(df_data.values)
        
#         self.data_x = data[b1:b2]
#         self.data_y = data[b1:b2]
    
#     def __getitem__(self, index):
#         s_begin = index; s_end = s_begin + self.seq_len
#         r_begin = s_end; r_end = r_begin + self.pred_len
#         return torch.tensor(self.data_x[s_begin:s_end], dtype=torch.float32), \
#                torch.tensor(self.data_y[r_begin:r_end], dtype=torch.float32)

#     def __len__(self): return len(self.data_x) - self.seq_len - self.pred_len + 1

# # ==========================================
# # 4. ä¸»è®­ç»ƒå¾ªçŽ¯ 
# # ==========================================
# def main():
#     if "LOCAL_RANK" in os.environ:
#         rank = int(os.environ["LOCAL_RANK"]); torch.cuda.set_device(rank)
#         dist.init_process_group("nccl"); world_size = dist.get_world_size()
#     else: rank = 0; world_size = 1; torch.cuda.set_device(0); dist.init_process_group("gloo", rank=0, world_size=1)

#     log_file = "itransformer_huber_log.csv"
#     dataset_path_file = ".kaggle_path.tmp"
    
#     if rank == 0:
#         print("ðŸ“¥ Initializing...")
#         path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
#         with open(dataset_path_file, "w") as f: f.write(path)
#         if not os.path.exists(log_file):
#             with open(log_file, "w") as f: f.write("Epoch,Train_HuberLoss,Val_MSE\n")
    
#     if dist.is_initialized(): dist.barrier()
#     with open(dataset_path_file, "r") as f: base_path = f.read().strip()
#     csv_path = glob.glob(os.path.join(base_path, "**", "electricity.csv"), recursive=True)[0]

#     BS = 64 // world_size
#     train_ds = ElectricityDataset(csv_path, 'train')
#     val_ds = ElectricityDataset(csv_path, 'val')
#     train_loader = DataLoader(train_ds, batch_size=BS, sampler=DistributedSampler(train_ds) if world_size > 1 else None, shuffle=(world_size==1), num_workers=4)
#     val_loader = DataLoader(val_ds, batch_size=BS, sampler=DistributedSampler(val_ds, shuffle=False) if world_size > 1 else None, shuffle=False)

#     model = iTransformer(num_vars=321, d_model=128, n_heads=8, e_layers=3).to(rank)
#     if world_size > 1: model = DDP(model, device_ids=[rank])

#     optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30) 
    
#     # ðŸŒŸ é‡ç‚¹ï¼šè®­ç»ƒç”¨ HuberLoss
#     criterion = nn.HuberLoss() 

#     if rank == 0: print(f"ðŸš€ iTransformer (HuberLoss) Started | Epochs=30")
    
#     for epoch in range(30): 
#         if world_size > 1: train_loader.sampler.set_epoch(epoch)
#         model.train()
#         train_loss = 0
#         for bx, by in train_loader:
#             bx, by = bx.to(rank), by.to(rank)
#             optimizer.zero_grad(set_to_none=True)
#             with autocast(device_type='cuda', dtype=torch.bfloat16):
#                 pred = model(bx)
#                 loss = criterion(pred, by) 
#             loss.backward()
#             optimizer.step()
#             train_loss += loss.item()
            
#         model.eval()
#         v_mse = 0; count = 0
#         with torch.no_grad():
#             for bx, by in val_loader:
#                 bx, by = bx.to(rank), by.to(rank)
#                 pred = model(bx)
#                 v_mse += F.mse_loss(pred, by).item() 
#                 count += 1
                
#         avg_mse = torch.tensor(v_mse/count).to(rank)
#         if world_size > 1: dist.all_reduce(avg_mse, op=dist.ReduceOp.SUM); avg_mse /= world_size

#         if rank == 0:
#             avg_train_loss = train_loss/len(train_loader)
#             print(f"âœ… Epoch {epoch+1}/30 | Train HuberLoss: {avg_train_loss:.4f} | Val MSE: {avg_mse.item():.4f}")
#             with open(log_file, "a") as f: f.write(f"{epoch+1},{avg_train_loss:.4f},{avg_mse.item():.4f}\n")
#             torch.save(model.state_dict(), "itransformer_huber_best.pth")
        
#         scheduler.step()

# if __name__ == "__main__": main()

import os, math, random, time, glob, torch, kagglehub
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
from sklearn.preprocessing import StandardScaler

# ==========================================
# 0. çŽ¯å¢ƒé…ç½®ä¸ŽåŠ é€Ÿ
# ==========================================
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# ==========================================
# 1. æ ¸å¿ƒç»„ä»¶ï¼šRevIN 
# ==========================================
class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True):
        super(RevIN, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.affine = affine
        if self.affine: self._init_params()

    def _init_params(self):
        self.affine_weight = nn.Parameter(torch.ones(self.num_features))
        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

    def forward(self, x, mode:str):
        if mode == 'norm':
            self._get_statistics(x)
            x = self._normalize(x)
        elif mode == 'denorm':
            x = self._denormalize(x)
        return x

    def _get_statistics(self, x):
        dim2reduce = tuple(range(len(x.shape)-1))
        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()

    def _normalize(self, x):
        x = (x - self.mean) / self.stdev
        if self.affine: x = x * self.affine_weight + self.affine_bias
        return x

    def _denormalize(self, x):
        if self.affine:
            x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)
        x = x * self.stdev + self.mean
        return x

# ==========================================
# 2. åŽŸç”Ÿ iTransformer æž¶æž„
# ==========================================
class iTransformer(nn.Module):
    def __init__(self, seq_len=96, pred_len=96, num_vars=321, d_model=128, n_heads=8, e_layers=3, d_ff=512, dropout=0.1):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.num_vars = num_vars
        
        self.revin = RevIN(num_vars)
        self.enc_embedding = nn.Linear(seq_len, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff, 
            dropout=dropout, batch_first=True, norm_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)
        self.projector = nn.Linear(d_model, pred_len)

    def forward(self, x):
        x = self.revin(x, 'norm')
        # [B, L, N] -> [B, N, L]  Inverted ç¿»è½¬ï¼
        x = x.permute(0, 2, 1)
        
        enc_out = self.enc_embedding(x)
        enc_out = self.encoder(enc_out)
        dec_out = self.projector(enc_out)
        
        dec_out = dec_out.permute(0, 2, 1)
        dec_out = self.revin(dec_out, 'denorm')
        return dec_out

# ==========================================
# 3. æ•°æ®é›†å®šä¹‰ 
# ==========================================
class ElectricityDataset(Dataset):
    def __init__(self, csv_path, flag='train'):
        self.seq_len = 96; self.pred_len = 96
        df_raw = pd.read_csv(csv_path)
        df_data = df_raw.iloc[:, 1:] 
        n = len(df_data)
        num_train = int(n * 0.7); num_test = int(n * 0.2); num_val = n - num_train - num_test
        
        border1s = [0, num_train - self.seq_len, n - num_test - self.seq_len]
        border2s = [num_train, num_train + num_val, n]
        f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
        b1, b2 = border1s[f_idx], border2s[f_idx]
        
        self.scaler = StandardScaler()
        train_data = df_data.iloc[border1s[0]:border2s[0]]
        self.scaler.fit(train_data.values)
        data = self.scaler.transform(df_data.values)
        
        self.data_x = data[b1:b2]
        self.data_y = data[b1:b2]
    
    def __getitem__(self, index):
        s_begin = index; s_end = s_begin + self.seq_len
        r_begin = s_end; r_end = r_begin + self.pred_len
        return torch.tensor(self.data_x[s_begin:s_end], dtype=torch.float32), \
               torch.tensor(self.data_y[r_begin:r_end], dtype=torch.float32)

    def __len__(self): return len(self.data_x) - self.seq_len - self.pred_len + 1

# ==========================================
# 4. ä¸»è®­ç»ƒå¾ªçŽ¯ 
# ==========================================
def main():
    if "LOCAL_RANK" in os.environ:
        rank = int(os.environ["LOCAL_RANK"]); torch.cuda.set_device(rank)
        dist.init_process_group("nccl"); world_size = dist.get_world_size()
    else: rank = 0; world_size = 1; torch.cuda.set_device(0); dist.init_process_group("gloo", rank=0, world_size=1)

    log_file = "itransformer_mse_log.csv"
    dataset_path_file = ".kaggle_path.tmp"
    
    if rank == 0:
        print("ðŸ“¥ Initializing...")
        path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
        with open(dataset_path_file, "w") as f: f.write(path)
        if not os.path.exists(log_file):
            with open(log_file, "w") as f: f.write("Epoch,Train_MSE,Val_MSE\n")
    
    if dist.is_initialized(): dist.barrier()
    with open(dataset_path_file, "r") as f: base_path = f.read().strip()
    csv_path = glob.glob(os.path.join(base_path, "**", "electricity.csv"), recursive=True)[0]

    BS = 64 // world_size
    train_ds = ElectricityDataset(csv_path, 'train')
    val_ds = ElectricityDataset(csv_path, 'val')
    train_loader = DataLoader(train_ds, batch_size=BS, sampler=DistributedSampler(train_ds) if world_size > 1 else None, shuffle=(world_size==1), num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=BS, sampler=DistributedSampler(val_ds, shuffle=False) if world_size > 1 else None, shuffle=False)

    model = iTransformer(num_vars=321, d_model=128, n_heads=8, e_layers=3).to(rank)
    if world_size > 1: model = DDP(model, device_ids=[rank])

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30) 
    
    # ðŸŒŸ é‡ç‚¹ï¼šè®­ç»ƒæ›¿æ¢ä¸ºåŽŸç”Ÿ MSELoss
    criterion = nn.MSELoss() 

    if rank == 0: print(f"ðŸš€ iTransformer (Pure MSE) Started | Epochs=30")
    
    for epoch in range(30): 
        if world_size > 1: train_loader.sampler.set_epoch(epoch)
        model.train()
        train_loss = 0
        for bx, by in train_loader:
            bx, by = bx.to(rank), by.to(rank)
            optimizer.zero_grad(set_to_none=True)
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                pred = model(bx)
                loss = criterion(pred, by) 
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        model.eval()
        v_mse = 0; count = 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(rank), by.to(rank)
                pred = model(bx)
                v_mse += F.mse_loss(pred, by).item() 
                count += 1
                
        avg_mse = torch.tensor(v_mse/count).to(rank)
        if world_size > 1: dist.all_reduce(avg_mse, op=dist.ReduceOp.SUM); avg_mse /= world_size

        if rank == 0:
            avg_train_loss = train_loss/len(train_loader)
            print(f"âœ… Epoch {epoch+1}/30 | Train MSE: {avg_train_loss:.4f} | Val MSE: {avg_mse.item():.4f}")
            with open(log_file, "a") as f: f.write(f"{epoch+1},{avg_train_loss:.4f},{avg_mse.item():.4f}\n")
            torch.save(model.state_dict(), "itransformer_mse_best.pth")
        
        scheduler.step()

if __name__ == "__main__": main()