#ç¬¬ä¸€ä»½ä»£ç æ˜¯geminiå†™çš„ï¼Œç¬¬äºŒä»½æ˜¯å®˜æ–¹çš„, ä½†æ˜¯geminiåšäº†ç®€å•è°ƒæ•´ï¼Œä¸¤ä»½çš„test mseéƒ½æ˜¯0.14=0.15ä¹‹é—´ï¼Œæ²¡æœ‰æˆ‘çš„swinifoldçš„0.13-0.135å¥½


(base) root@ubuntu22:~# torchrun --nproc_per_node=4 test2.py
[2026-02-15 00:44:02,645] torch.distributed.run: [WARNING] 
[2026-02-15 00:44:02,645] torch.distributed.run: [WARNING] *****************************************
[2026-02-15 00:44:02,645] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2026-02-15 00:44:02,645] torch.distributed.run: [WARNING] *****************************************
ase) root@ubuntu22:~#  python -m torch.distributed.run --nproc_per_node=4 test2.py
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
ğŸ“¥ Initializing...
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
ğŸš€ iTransformer (HuberLoss) Started | Epochs=30
âœ… Epoch 1/30 | Train Huber: 0.1036 | Val MSE: 0.1649
âœ… Epoch 2/30 | Train Huber: 0.0789 | Val MSE: 0.1471
âœ… Epoch 3/30 | Train Huber: 0.0724 | Val MSE: 0.1381
âœ… Epoch 4/30 | Train Huber: 0.0694 | Val MSE: 0.1322
âœ… Epoch 5/30 | Train Huber: 0.0676 | Val MSE: 0.1296
âœ… Epoch 6/30 | Train Huber: 0.0660 | Val MSE: 0.1295
âœ… Epoch 7/30 | Train Huber: 0.0653 | Val MSE: 0.1292
âœ… Epoch 8/30 | Train Huber: 0.0641 | Val MSE: 0.1277
âœ… Epoch 9/30 | Train Huber: 0.0630 | Val MSE: 0.1262
âœ… Epoch 10/30 | Train Huber: 0.0630 | Val MSE: 0.1293
âœ… Epoch 11/30 | Train Huber: 0.0617 | Val MSE: 0.1289
âœ… Epoch 12/30 | Train Huber: 0.0617 | Val MSE: 0.1242
âœ… Epoch 13/30 | Train Huber: 0.0608 | Val MSE: 0.1262
âœ… Epoch 14/30 | Train Huber: 0.0605 | Val MSE: 0.1256
âœ… Epoch 15/30 | Train Huber: 0.0600 | Val MSE: 0.1248
âœ… Epoch 16/30 | Train Huber: 0.0590 | Val MSE: 0.1256
âœ… Epoch 17/30 | Train Huber: 0.0580 | Val MSE: 0.1246
âœ… Epoch 18/30 | Train Huber: 0.0578 | Val MSE: 0.1259
âœ… Epoch 19/30 | Train Huber: 0.0572 | Val MSE: 0.1236
âœ… Epoch 20/30 | Train Huber: 0.0571 | Val MSE: 0.1249
âœ… Epoch 21/30 | Train Huber: 0.0565 | Val MSE: 0.1242
âœ… Epoch 22/30 | Train Huber: 0.0562 | Val MSE: 0.1238
âœ… Epoch 23/30 | Train Huber: 0.0561 | Val MSE: 0.1227
âœ… Epoch 24/30 | Train Huber: 0.0556 | Val MSE: 0.1243
âœ… Epoch 25/30 | Train Huber: 0.0555 | Val MSE: 0.1225
âœ… Epoch 26/30 | Train Huber: 0.0556 | Val MSE: 0.1233
âœ… Epoch 27/30 | Train Huber: 0.0556 | Val MSE: 0.1237
âœ… Epoch 28/30 | Train Huber: 0.0552 | Val MSE: 0.1235
âœ… Epoch 29/30 | Train Huber: 0.0553 | Val MSE: 0.1230
âœ… Epoch 30/30 | Train Huber: 0.0553 | Val MSE: 0.1232

ğŸ Training Finished. Evaluating on Test Set...
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
ğŸ† FINAL TEST RESULT:
   - Test MSE: 0.14889
   - Test MAE: 0.24216
(base) root@ubuntu22:~# 






import os, math, random, time, glob, torch, kagglehub
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
from sklearn.preprocessing import StandardScaler

# ==========================================
# 0. ç¯å¢ƒé…ç½®ä¸åŠ é€Ÿ
# ==========================================
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# ==========================================
# 1. æ ¸å¿ƒç»„ä»¶ï¼šRevIN 
# ==========================================
class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True):
        super(RevIN, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.affine = affine
        if self.affine: self._init_params()

    def _init_params(self):
        self.affine_weight = nn.Parameter(torch.ones(self.num_features))
        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

    def forward(self, x, mode:str):
        if mode == 'norm':
            self._get_statistics(x)
            x = self._normalize(x)
        elif mode == 'denorm':
            x = self._denormalize(x)
        return x

    def _get_statistics(self, x):
        dim2reduce = tuple(range(len(x.shape)-1))
        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()

    def _normalize(self, x):
        x = (x - self.mean) / self.stdev
        if self.affine: x = x * self.affine_weight + self.affine_bias
        return x

    def _denormalize(self, x):
        if self.affine:
            x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)
        x = x * self.stdev + self.mean
        return x

# ==========================================
# 2. åŸç”Ÿ iTransformer æ¶æ„
# ==========================================
class iTransformer(nn.Module):
    def __init__(self, seq_len=96, pred_len=96, num_vars=321, d_model=128, n_heads=8, e_layers=3, d_ff=512, dropout=0.1):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.num_vars = num_vars
        
        self.revin = RevIN(num_vars)
        self.enc_embedding = nn.Linear(seq_len, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff, 
            dropout=dropout, batch_first=True, norm_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)
        self.projector = nn.Linear(d_model, pred_len)

    def forward(self, x):
        x = self.revin(x, 'norm')
        # [B, L, N] -> [B, N, L]  Inverted ç¿»è½¬ï¼
        x = x.permute(0, 2, 1)
        
        enc_out = self.enc_embedding(x)
        enc_out = self.encoder(enc_out)
        dec_out = self.projector(enc_out)
        
        dec_out = dec_out.permute(0, 2, 1)
        dec_out = self.revin(dec_out, 'denorm')
        return dec_out

# ==========================================
# 3. æ•°æ®é›†å®šä¹‰ 
# ==========================================
class ElectricityDataset(Dataset):
    def __init__(self, csv_path, flag='train'):
        self.seq_len = 96; self.pred_len = 96
        df_raw = pd.read_csv(csv_path)
        df_data = df_raw.iloc[:, 1:] 
        n = len(df_data)
        num_train = int(n * 0.7); num_test = int(n * 0.2); num_val = n - num_train - num_test
        
        border1s = [0, num_train - self.seq_len, n - num_test - self.seq_len]
        border2s = [num_train, num_train + num_val, n]
        f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
        b1, b2 = border1s[f_idx], border2s[f_idx]
        
        self.scaler = StandardScaler()
        train_data = df_data.iloc[border1s[0]:border2s[0]]
        self.scaler.fit(train_data.values)
        data = self.scaler.transform(df_data.values)
        
        self.data_x = data[b1:b2]
        self.data_y = data[b1:b2]
    
    def __getitem__(self, index):
        s_begin = index; s_end = s_begin + self.seq_len
        r_begin = s_end; r_end = r_begin + self.pred_len
        return torch.tensor(self.data_x[s_begin:s_end], dtype=torch.float32), \
               torch.tensor(self.data_y[r_begin:r_end], dtype=torch.float32)

    def __len__(self): return len(self.data_x) - self.seq_len - self.pred_len + 1

# ==========================================
# 4. ä¸»è®­ç»ƒå¾ªç¯ 
# ==========================================
def main():
    if "LOCAL_RANK" in os.environ:
        rank = int(os.environ["LOCAL_RANK"]); torch.cuda.set_device(rank)
        dist.init_process_group("nccl"); world_size = dist.get_world_size()
    else: rank = 0; world_size = 1; torch.cuda.set_device(0); dist.init_process_group("gloo", rank=0, world_size=1)

    log_file = "itransformer_huber_log.csv"
    dataset_path_file = ".kaggle_path.tmp"
    
    if rank == 0:
        print("ğŸ“¥ Initializing...")
        path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
        with open(dataset_path_file, "w") as f: f.write(path)
        if not os.path.exists(log_file):
            with open(log_file, "w") as f: f.write("Epoch,Train_HuberLoss,Val_MSE\n")
    
    if dist.is_initialized(): dist.barrier()
    with open(dataset_path_file, "r") as f: base_path = f.read().strip()
    csv_path = glob.glob(os.path.join(base_path, "**", "electricity.csv"), recursive=True)[0]

    BS = 64 // world_size
    train_ds = ElectricityDataset(csv_path, 'train')
    val_ds = ElectricityDataset(csv_path, 'val')
    test_ds = ElectricityDataset(csv_path, 'test') # ğŸŒŸ å¢åŠ æµ‹è¯•é›†
    
    train_loader = DataLoader(train_ds, batch_size=BS, sampler=DistributedSampler(train_ds) if world_size > 1 else None, shuffle=(world_size==1), num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=BS, sampler=DistributedSampler(val_ds, shuffle=False) if world_size > 1 else None, shuffle=False)
    test_loader = DataLoader(test_ds, batch_size=BS, shuffle=False) # ğŸŒŸ å¢åŠ æµ‹è¯• DataLoader

    model = iTransformer(num_vars=321, d_model=128, n_heads=8, e_layers=3).to(rank)
    if world_size > 1: model = DDP(model, device_ids=[rank])

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30) 
    
    # ğŸŒŸ é‡ç‚¹ï¼šè®­ç»ƒç”¨ HuberLoss
    criterion_train = nn.HuberLoss(delta=1.0) 

    if rank == 0: print(f"ğŸš€ iTransformer (HuberLoss) Started | Epochs=30")
    
    best_val_mse = float('inf')

    for epoch in range(30): 
        if world_size > 1: train_loader.sampler.set_epoch(epoch)
        model.train()
        train_loss = 0
        for bx, by in train_loader:
            bx, by = bx.to(rank), by.to(rank)
            optimizer.zero_grad(set_to_none=True)
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                pred = model(bx)
                loss = criterion_train(pred, by) 
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        model.eval()
        v_mse = 0; count = 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(rank), by.to(rank)
                pred = model(bx)
                v_mse += F.mse_loss(pred, by).item() 
                count += 1
                
        avg_mse_val = torch.tensor(v_mse/count).to(rank)
        if world_size > 1: dist.all_reduce(avg_mse_val, op=dist.ReduceOp.SUM); avg_mse_val /= world_size

        if rank == 0:
            avg_train_loss = train_loss/len(train_loader)
            print(f"âœ… Epoch {epoch+1}/30 | Train Huber: {avg_train_loss:.4f} | Val MSE: {avg_mse_val.item():.4f}")
            with open(log_file, "a") as f: f.write(f"{epoch+1},{avg_train_loss:.4f},{avg_mse_val.item():.4f}\n")
            
            # ğŸŒŸ åªåœ¨éªŒè¯é›† MSE é™ä½æ—¶ä¿å­˜æ¨¡å‹
            if avg_mse_val.item() < best_val_mse:
                best_val_mse = avg_mse_val.item()
                torch.save(model.module.state_dict() if world_size > 1 else model.state_dict(), "itransformer_best.pth")
        
        scheduler.step()

    # --- ç»ˆææµ‹è¯•å¤§è€ƒ (Rank 0 è´Ÿè´£) ---
    dist.barrier()
    if rank == 0:
        print("\nğŸ Training Finished. Evaluating on Test Set...")
        # åŠ è½½éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æƒé‡
        test_model = iTransformer(num_vars=321, d_model=128, n_heads=8, e_layers=3).to(rank)
        test_model.load_state_dict(torch.load("itransformer_best.pth"))
        test_model.eval()
        
        t_mse, t_mae, t_count = 0, 0, 0
        with torch.no_grad():
            for bx, by in test_loader:
                bx, by = bx.to(rank), by.to(rank)
                pred = test_model(bx)
                t_mse += F.mse_loss(pred, by).item()
                t_mae += F.l1_loss(pred, by).item()
                t_count += 1
        
        print(f"ğŸ† FINAL TEST RESULT:")
        print(f"   - Test MSE: {t_mse/t_count:.5f}")
        print(f"   - Test MAE: {t_mae/t_count:.5f}")

    if dist.is_initialized(): dist.destroy_process_group()

if __name__ == "__main__": 
    main()

(base) root@ubuntu22:~# python -m torch.distributed.run --nproc_per_node=4 itrasformer_off.py
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
ğŸš€ Official iTransformer (Huber Engine) Start | 4 GPUs Running
âœ… Epoch 1/30 | Train Huber: 0.1046 | Val MSE: 0.1664
âœ… Epoch 2/30 | Train Huber: 0.0845 | Val MSE: 0.1557
âœ… Epoch 3/30 | Train Huber: 0.0794 | Val MSE: 0.1474
âœ… Epoch 4/30 | Train Huber: 0.0769 | Val MSE: 0.1424
âœ… Epoch 5/30 | Train Huber: 0.0753 | Val MSE: 0.1392
âœ… Epoch 6/30 | Train Huber: 0.0736 | Val MSE: 0.1371
âœ… Epoch 7/30 | Train Huber: 0.0729 | Val MSE: 0.1364
âœ… Epoch 8/30 | Train Huber: 0.0716 | Val MSE: 0.1344
âœ… Epoch 9/30 | Train Huber: 0.0706 | Val MSE: 0.1326
âœ… Epoch 10/30 | Train Huber: 0.0702 | Val MSE: 0.1318
âœ… Epoch 11/30 | Train Huber: 0.0691 | Val MSE: 0.1340
âœ… Epoch 12/30 | Train Huber: 0.0694 | Val MSE: 0.1306
âœ… Epoch 13/30 | Train Huber: 0.0683 | Val MSE: 0.1306
âœ… Epoch 14/30 | Train Huber: 0.0683 | Val MSE: 0.1299
âœ… Epoch 15/30 | Train Huber: 0.0681 | Val MSE: 0.1299
âœ… Epoch 16/30 | Train Huber: 0.0673 | Val MSE: 0.1290
âœ… Epoch 17/30 | Train Huber: 0.0665 | Val MSE: 0.1295
âœ… Epoch 18/30 | Train Huber: 0.0661 | Val MSE: 0.1285
âœ… Epoch 19/30 | Train Huber: 0.0659 | Val MSE: 0.1294
âœ… Epoch 20/30 | Train Huber: 0.0658 | Val MSE: 0.1290
âœ… Epoch 21/30 | Train Huber: 0.0649 | Val MSE: 0.1281
âœ… Epoch 22/30 | Train Huber: 0.0646 | Val MSE: 0.1289
âœ… Epoch 23/30 | Train Huber: 0.0644 | Val MSE: 0.1279
âœ… Epoch 24/30 | Train Huber: 0.0639 | Val MSE: 0.1291
âœ… Epoch 25/30 | Train Huber: 0.0639 | Val MSE: 0.1277
âœ… Epoch 26/30 | Train Huber: 0.0638 | Val MSE: 0.1284
âœ… Epoch 27/30 | Train Huber: 0.0638 | Val MSE: 0.1277
âœ… Epoch 28/30 | Train Huber: 0.0632 | Val MSE: 0.1280
âœ… Epoch 29/30 | Train Huber: 0.0634 | Val MSE: 0.1277
âœ… Epoch 30/30 | Train Huber: 0.0636 | Val MSE: 0.1278

ğŸ Training Finished. Evaluating on Test Set...
ğŸ† TEST RESULTS: MSE: 0.14905 | MAE: 0.23984



import os, math, random, time, glob, torch, kagglehub
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
from sklearn.preprocessing import StandardScaler

# ==========================================
# 0. ç¯å¢ƒé…ç½®ä¸åŠ é€Ÿ
# ==========================================
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# ==============================================================================
# ğŸ› ï¸ 1. å®˜æ–¹åº•å±‚ç»„ä»¶ (ç”± layers/ æ–‡ä»¶å¤¹æå–)
# ==============================================================================
class DataEmbedding_inverted(nn.Module):
    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):
        super(DataEmbedding_inverted, self).__init__()
        self.value_embedding = nn.Linear(c_in, d_model)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x, x_mark):
        x = x.permute(0, 2, 1) # [B, L, N] -> [B, N, L] åºåˆ—å€’è½¬çš„æ ¸å¿ƒï¼
        if x_mark is None:
            x = self.value_embedding(x)
        else:
            x = self.value_embedding(torch.cat([x, x_mark.permute(0, 2, 1)], 1))
        return self.dropout(x)

class FullAttention(nn.Module):
    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):
        super(FullAttention, self).__init__()
        self.scale = scale
        self.mask_flag = mask_flag
        self.output_attention = output_attention
        self.dropout = nn.Dropout(attention_dropout)

    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):
        B, L, H, E = queries.shape
        _, S, _, D = keys.shape
        scale = self.scale or 1. / math.sqrt(E)
        scores = torch.einsum("blhe,bshe->blhs", queries, keys)
        if self.mask_flag and attn_mask is not None:
            scores.masked_fill_(attn_mask.mask, -np.inf)
        A = self.dropout(torch.softmax(scale * scores, dim=-1))
        V = torch.einsum("blhs,bshe->blhe", A, values)
        return (V.contiguous(), A) if self.output_attention else (V.contiguous(), None)

class AttentionLayer(nn.Module):
    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):
        super(AttentionLayer, self).__init__()
        d_keys = d_keys or (d_model // n_heads)
        d_values = d_values or (d_model // n_heads)
        self.inner_attention = attention
        self.query_projection = nn.Linear(d_model, d_keys * n_heads)
        self.key_projection = nn.Linear(d_model, d_keys * n_heads)
        self.value_projection = nn.Linear(d_model, d_values * n_heads)
        self.out_projection = nn.Linear(d_values * n_heads, d_model)
        self.n_heads = n_heads

    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):
        B, L, _ = queries.shape; H = self.n_heads
        queries = self.query_projection(queries).view(B, L, H, -1)
        keys = self.key_projection(keys).view(B, L, H, -1)
        values = self.value_projection(values).view(B, L, H, -1)
        out, attn = self.inner_attention(queries, keys, values, attn_mask, tau, delta)
        return self.out_projection(out.view(B, L, -1)), attn

class EncoderLayer(nn.Module):
    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation="relu"):
        super(EncoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model
        self.attention = attention
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
        self.norm1 = nn.LayerNorm(d_model); self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = F.relu if activation == "relu" else F.gelu

    def forward(self, x, attn_mask=None, tau=None, delta=None):
        new_x, attn = self.attention(x, x, x, attn_mask=attn_mask, tau=tau, delta=delta)
        x = x + self.dropout(new_x)
        y = x = self.norm1(x)
        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        y = self.dropout(self.conv2(y).transpose(-1, 1))
        return self.norm2(x + y), attn

class Encoder(nn.Module):
    def __init__(self, attn_layers, norm_layer=None):
        super(Encoder, self).__init__()
        self.attn_layers = nn.ModuleList(attn_layers)
        self.norm = norm_layer
    def forward(self, x, attn_mask=None, tau=None, delta=None):
        attns = []
        for attn_layer in self.attn_layers:
            x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
            attns.append(attn)
        if self.norm is not None: x = self.norm(x)
        return x, attns

# ==============================================================================
# ğŸ† 2. å®˜æ–¹æ ¸å¿ƒä¸»ç±»: Model (iTransformer)
# ==============================================================================
class Model(nn.Module):
    """
    Paper link: https://arxiv.org/abs/2310.06625
    """
    def __init__(self, configs):
        super(Model, self).__init__()
        self.task_name = configs.task_name
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        
        # Embedding
        self.enc_embedding = DataEmbedding_inverted(configs.seq_len, configs.d_model, configs.embed, configs.freq,
                                                    configs.dropout)
        # Encoder
        self.encoder = Encoder(
            [
                EncoderLayer(
                    AttentionLayer(
                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,
                                      output_attention=False), configs.d_model, configs.n_heads),
                    configs.d_model,
                    configs.d_ff,
                    dropout=configs.dropout,
                    activation=configs.activation
                ) for l in range(configs.e_layers)
            ],
            norm_layer=torch.nn.LayerNorm(configs.d_model)
        )
        
        # Decoder
        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':
            self.projection = nn.Linear(configs.d_model, configs.pred_len, bias=True)
            
        if self.task_name == 'classification':
            self.act = F.gelu
            self.dropout = nn.Dropout(configs.dropout)
            self.projection = nn.Linear(configs.d_model * configs.enc_in, configs.num_class)

    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):
        # Normalization from Non-stationary Transformer
        means = x_enc.mean(1, keepdim=True).detach()
        x_enc = x_enc - means
        stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
        x_enc /= stdev

        _, _, N = x_enc.shape

        # Embedding
        enc_out = self.enc_embedding(x_enc, x_mark_enc)
        enc_out, attns = self.encoder(enc_out, attn_mask=None)

        dec_out = self.projection(enc_out).permute(0, 2, 1)[:, :, :N]
        
        # De-Normalization from Non-stationary Transformer
        dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
        dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
        return dec_out

    def forward(self, x_enc, x_mark_enc=None, x_dec=None, x_mark_dec=None, mask=None):
        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':
            dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
            return dec_out[:, -self.pred_len:, :]  # [B, L, D]
        return None

# ==========================================
# 3. æ•°æ®é›†ä¸ Configs
# ==========================================
class ElectricityDataset(Dataset):
    def __init__(self, csv_path, flag='train'):
        self.seq_len = 96; self.pred_len = 96
        df_raw = pd.read_csv(csv_path)
        df_data = df_raw.iloc[:, 1:] 
        n = len(df_data)
        num_train = int(n * 0.7); num_test = int(n * 0.2); num_val = n - num_train - num_test
        
        border1s = [0, num_train - self.seq_len, n - num_test - self.seq_len]
        border2s = [num_train, num_train + num_val, n]
        f_idx = {'train': 0, 'val': 1, 'test': 2}[flag]
        b1, b2 = border1s[f_idx], border2s[f_idx]
        
        self.scaler = StandardScaler()
        train_data = df_data.iloc[border1s[0]:border2s[0]]
        self.scaler.fit(train_data.values)
        data = self.scaler.transform(df_data.values)
        
        self.data_x = data[b1:b2]
        self.data_y = data[b1:b2]
    
    def __getitem__(self, index):
        s_begin = index; s_end = s_begin + self.seq_len
        r_begin = s_end; r_end = r_begin + self.pred_len
        return torch.tensor(self.data_x[s_begin:s_end], dtype=torch.float32), \
               torch.tensor(self.data_y[r_begin:r_end], dtype=torch.float32)

    def __len__(self): return len(self.data_x) - self.seq_len - self.pred_len + 1

class Configs:
    task_name = 'long_term_forecast'
    seq_len = 96; pred_len = 96
    d_model = 128; embed = 'fixed'; freq = 'h'
    dropout = 0.1; factor = 3; n_heads = 8; d_ff = 512
    activation = 'gelu'; e_layers = 3; enc_in = 321; num_class = 1

# ==========================================
# 4. è®­ç»ƒä¸æµ‹è¯•å¤§è€ƒå¼•æ“
# ==========================================
def main():
    if "LOCAL_RANK" in os.environ:
        rank = int(os.environ["LOCAL_RANK"]); torch.cuda.set_device(rank)
        dist.init_process_group("nccl"); world_size = dist.get_world_size()
    else: rank = 0; world_size = 1; torch.cuda.set_device(0); dist.init_process_group("gloo", rank=0, world_size=1)

    dataset_path_file = ".kaggle_path.tmp"
    if rank == 0:
        path = kagglehub.dataset_download("tylerfarnan/itransformer-datasets")
        with open(dataset_path_file, "w") as f: f.write(path)
    
    if dist.is_initialized(): dist.barrier()
    
    with open(dataset_path_file, "r") as f: base_path = f.read().strip()
    csv_path = glob.glob(os.path.join(base_path, "**", "electricity.csv"), recursive=True)[0]

    BS = 64 // world_size
    train_ds = ElectricityDataset(csv_path, 'train')
    val_ds = ElectricityDataset(csv_path, 'val')
    test_ds = ElectricityDataset(csv_path, 'test') 
    
    train_loader = DataLoader(train_ds, batch_size=BS, sampler=DistributedSampler(train_ds) if world_size > 1 else None, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=BS, shuffle=False)
    test_loader = DataLoader(test_ds, batch_size=BS, shuffle=False)

    model = Model(Configs()).to(rank)
    if world_size > 1: model = DDP(model, device_ids=[rank])

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30) 
    criterion_train = nn.HuberLoss(delta=1.0) 

    if rank == 0: print(f"ğŸš€ Official iTransformer (Huber Engine) Start | 4 GPUs Running")
    
    best_val_mse = float('inf')

    # --- è®­ç»ƒå¾ªç¯ ---
    for epoch in range(30): 
        if world_size > 1: train_loader.sampler.set_epoch(epoch)
        model.train()
        train_loss = 0
        for bx, by in train_loader:
            bx, by = bx.to(rank), by.to(rank)
            optimizer.zero_grad(set_to_none=True)
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                # iTransformer éœ€è¦ä¼ å…¥ x_mark_enc ç­‰å‚æ•°ï¼Œè¿™é‡Œä¼  None å³å¯è§¦å‘å†…éƒ¨æŠ•å½±
                pred = model(bx, None, None, None)
                loss = criterion_train(pred, by)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        # éªŒè¯å¾ªç¯
        model.eval()
        v_mse = 0; count = 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(rank), by.to(rank)
                pred = model(bx, None, None, None)
                v_mse += F.mse_loss(pred, by).item() 
                count += 1
        
        avg_mse_val = torch.tensor(v_mse/count).to(rank)
        if world_size > 1: dist.all_reduce(avg_mse_val, op=dist.ReduceOp.SUM); avg_mse_val /= world_size

        if rank == 0:
            print(f"âœ… Epoch {epoch+1}/30 | Train Huber: {train_loss/len(train_loader):.4f} | Val MSE: {avg_mse_val.item():.4f}")
            if avg_mse_val.item() < best_val_mse:
                best_val_mse = avg_mse_val.item()
                torch.save(model.module.state_dict() if world_size > 1 else model.state_dict(), "itransformer_official_best.pth")
        
        scheduler.step()

    # --- ç»ˆææµ‹è¯•å¤§è€ƒ (Rank 0 è´Ÿè´£) ---
    dist.barrier()
    if rank == 0:
        print("\nğŸ Training Finished. Evaluating on Test Set...")
        test_model = Model(Configs()).to(rank)
        test_model.load_state_dict(torch.load("itransformer_official_best.pth"))
        test_model.eval()
        
        t_mse, t_mae, t_count = 0, 0, 0
        with torch.no_grad():
            for bx, by in test_loader:
                bx, by = bx.to(rank), by.to(rank)
                pred = test_model(bx, None, None, None)
                t_mse += F.mse_loss(pred, by).item()
                t_mae += F.l1_loss(pred, by).item()
                t_count += 1
        
        print(f"ğŸ† TEST RESULTS: MSE: {t_mse/t_count:.5f} | MAE: {t_mae/t_count:.5f}")

    if dist.is_initialized(): dist.destroy_process_group()

if __name__ == "__main__": 
    main()












